{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Pima Indian dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import data\n",
    "from model import MLP\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "seed=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "#TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve imputed train and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentPath = os.getcwd()\n",
    "test_list = data.testImputed(currentPath=currentPath)\n",
    "train = data.trainImputed(currentPath=currentPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Outcome into two columns instead of 1: Diabetic and Not_Diabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For train\n",
    "train[\"Not Diabetic\"] = (train[\"Outcome\"] == 0).astype(int)\n",
    "train.rename(columns={\"Outcome\": \"Diabetic\"}, inplace=True)\n",
    "\n",
    "#For test\n",
    "for i in range(len(test_list)):\n",
    "    test_list[i][\"Not Diabetic\"] = (test_list[i][\"Outcome\"] == 0).astype(int)\n",
    "    test_list[i].rename(columns={\"Outcome\": \"Diabetic\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into target and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.as_matrix()[:,0:-2]\n",
    "y_train = train.as_matrix()[:, -2:]\n",
    "test_split_list = list()\n",
    "\n",
    "#First element is X, second element is y\n",
    "for i in range(len(test_list)):\n",
    "    test_split_list.append( [ test_list[i].as_matrix()[:,0:-2],\n",
    "                         test_list[i].as_matrix()[:,-2:] ] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_split_list[0][0]\n",
    "y_test = test_split_list[0][1]\n",
    "scale_X = StandardScaler()\n",
    "scale_X.fit(X_train)\n",
    "X_train_scaled = scale_X.transform(X_train)\n",
    "X_test_scaled = scale_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate and train a Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "reload(model)\n",
    "from model import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = X_train_scaled.shape[1]\n",
    "outputDim = y_train.shape[1]\n",
    "mlp = MLP()\n",
    "mlp.buildModel(neurons=[10,10,10,10], activations=[\"relu\", \"relu\", \"relu\", \"relu\"], \n",
    "               dropout=[0.5,0.5,0.5,0.5], inputDim=inputDim, outputDim=outputDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training loss: 0.706515908241, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 11 : Training loss: 0.704764485359, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 21 : Training loss: 0.702696740627, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 31 : Training loss: 0.700844526291, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 41 : Training loss: 0.697329103947, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 51 : Training loss: 0.696234583855, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 61 : Training loss: 0.692880511284, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 71 : Training loss: 0.689456522465, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 81 : Training loss: 0.690316081047, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 91 : Training loss: 0.683042347431, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 101 : Training loss: 0.687663972378, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 111 : Training loss: 0.680644869804, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 121 : Training loss: 0.677502691746, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 131 : Training loss: 0.666579425335, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 141 : Training loss: 0.673410415649, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 151 : Training loss: 0.669799625874, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 161 : Training loss: 0.662181317806, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 171 : Training loss: 0.669597387314, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 181 : Training loss: 0.663963735104, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 191 : Training loss: 0.649111390114, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 201 : Training loss: 0.642676055431, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 211 : Training loss: 0.604106128216, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 221 : Training loss: 0.618971765041, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 231 : Training loss: 0.599858343601, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 241 : Training loss: 0.60164141655, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 251 : Training loss: 0.610950529575, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 261 : Training loss: 0.606283009052, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 271 : Training loss: 0.595638692379, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 281 : Training loss: 0.630593121052, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 291 : Training loss: 0.582282960415, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 301 : Training loss: 0.54444038868, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 311 : Training loss: 0.624089837074, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 321 : Training loss: 0.58918941021, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 331 : Training loss: 0.562024235725, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 341 : Training loss: 0.623160421848, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 351 : Training loss: 0.633232355118, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 361 : Training loss: 0.572537302971, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 371 : Training loss: 0.559137105942, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 381 : Training loss: 0.530272126198, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 391 : Training loss: 0.552326858044, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 401 : Training loss: 0.586108326912, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 411 : Training loss: 0.584522724152, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 421 : Training loss: 0.522122323513, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 431 : Training loss: 0.565301239491, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 441 : Training loss: 0.560639321804, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 451 : Training loss: 0.523426830769, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 461 : Training loss: 0.594077289104, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 471 : Training loss: 0.542023003101, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 481 : Training loss: 0.538111269474, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 491 : Training loss: 0.576770186424, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 501 : Training loss: 0.571034789085, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 511 : Training loss: 0.533903717995, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 521 : Training loss: 0.498672753572, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 531 : Training loss: 0.621587574482, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 541 : Training loss: 0.549381732941, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 551 : Training loss: 0.590662658215, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 561 : Training loss: 0.537024199963, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 571 : Training loss: 0.602749288082, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 581 : Training loss: 0.589080810547, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 591 : Training loss: 0.56560498476, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 601 : Training loss: 0.611538589001, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 611 : Training loss: 0.602429628372, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 621 : Training loss: 0.544384598732, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 631 : Training loss: 0.485539168119, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 641 : Training loss: 0.593792974949, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 651 : Training loss: 0.561063945293, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 661 : Training loss: 0.580112814903, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 671 : Training loss: 0.603796601295, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 681 : Training loss: 0.568198263645, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 691 : Training loss: 0.488660573959, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 701 : Training loss: 0.531295418739, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 711 : Training loss: 0.496491789818, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 721 : Training loss: 0.615479290485, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 731 : Training loss: 0.586226403713, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 741 : Training loss: 0.510996639729, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 751 : Training loss: 0.522620558739, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 761 : Training loss: 0.545411288738, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 771 : Training loss: 0.520906686783, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 781 : Training loss: 0.537436246872, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 791 : Training loss: 0.556242406368, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 801 : Training loss: 0.530877232552, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 811 : Training loss: 0.486064225435, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 821 : Training loss: 0.558657884598, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 831 : Training loss: 0.523826062679, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 841 : Training loss: 0.475134432316, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 851 : Training loss: 0.446972668171, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 861 : Training loss: 0.571395874023, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 871 : Training loss: 0.626694440842, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 881 : Training loss: 0.525979578495, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 891 : Training loss: 0.525694787502, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 901 : Training loss: 0.615615785122, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 911 : Training loss: 0.564568161964, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 921 : Training loss: 0.626946508884, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 931 : Training loss: 0.496961355209, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 941 : Training loss: 0.479419827461, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 951 : Training loss: 0.566526472569, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 961 : Training loss: 0.475553333759, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 971 : Training loss: 0.532367289066, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 981 : Training loss: 0.586560368538, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 991 : Training loss: 0.620825231075, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1001 : Training loss: 0.554166197777, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1011 : Training loss: 0.523741304874, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1021 : Training loss: 0.432634741068, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1031 : Training loss: 0.570024907589, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1041 : Training loss: 0.571493804455, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1051 : Training loss: 0.535321354866, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1061 : Training loss: 0.523790240288, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1071 : Training loss: 0.512268304825, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1081 : Training loss: 0.554431676865, \n",
      " test accuracy : 0.742857158184\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1091 : Training loss: 0.55999815464, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1101 : Training loss: 0.580728054047, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1111 : Training loss: 0.58830499649, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1121 : Training loss: 0.421236753464, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1131 : Training loss: 0.529187977314, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1141 : Training loss: 0.557278335094, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1151 : Training loss: 0.50847786665, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1161 : Training loss: 0.523816227913, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1171 : Training loss: 0.5902312994, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1181 : Training loss: 0.556781291962, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1191 : Training loss: 0.48606967926, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1201 : Training loss: 0.606019198895, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1211 : Training loss: 0.497242242098, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1221 : Training loss: 0.601434767246, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1231 : Training loss: 0.519243240356, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1241 : Training loss: 0.523465633392, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1251 : Training loss: 0.525522351265, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1261 : Training loss: 0.47980850935, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1271 : Training loss: 0.49000865221, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1281 : Training loss: 0.546639680862, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1291 : Training loss: 0.564438343048, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1301 : Training loss: 0.483279973269, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1311 : Training loss: 0.531393647194, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1321 : Training loss: 0.516859591007, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1331 : Training loss: 0.532243609428, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1341 : Training loss: 0.545834422112, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1351 : Training loss: 0.53520822525, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1361 : Training loss: 0.591417193413, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1371 : Training loss: 0.475660711527, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1381 : Training loss: 0.544931173325, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1391 : Training loss: 0.508010089397, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1401 : Training loss: 0.473767846823, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1411 : Training loss: 0.553735136986, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1421 : Training loss: 0.61473095417, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1431 : Training loss: 0.522203505039, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1441 : Training loss: 0.558483183384, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1451 : Training loss: 0.589933395386, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1461 : Training loss: 0.54110455513, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1471 : Training loss: 0.569707632065, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1481 : Training loss: 0.553934276104, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1491 : Training loss: 0.602800190449, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1501 : Training loss: 0.525565326214, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1511 : Training loss: 0.528651118279, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1521 : Training loss: 0.569357097149, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1531 : Training loss: 0.473291128874, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1541 : Training loss: 0.569984674454, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1551 : Training loss: 0.546912968159, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1561 : Training loss: 0.513356387615, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1571 : Training loss: 0.520154237747, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1581 : Training loss: 0.51995831728, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1591 : Training loss: 0.554239988327, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1601 : Training loss: 0.515807271004, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1611 : Training loss: 0.501390814781, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1621 : Training loss: 0.560752272606, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1631 : Training loss: 0.552908718586, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1641 : Training loss: 0.518032848835, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1651 : Training loss: 0.476338118315, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1661 : Training loss: 0.506104946136, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1671 : Training loss: 0.578707635403, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1681 : Training loss: 0.479639917612, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1691 : Training loss: 0.537447333336, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1701 : Training loss: 0.598440766335, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1711 : Training loss: 0.552420675755, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1721 : Training loss: 0.507374405861, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1731 : Training loss: 0.541304409504, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1741 : Training loss: 0.504262506962, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1751 : Training loss: 0.560841917992, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1761 : Training loss: 0.579151690006, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1771 : Training loss: 0.485666453838, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1781 : Training loss: 0.498705774546, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1791 : Training loss: 0.57466083765, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1801 : Training loss: 0.45426979661, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1811 : Training loss: 0.546130895615, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1821 : Training loss: 0.502517223358, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1831 : Training loss: 0.468002289534, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1841 : Training loss: 0.531000852585, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1851 : Training loss: 0.51003330946, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1861 : Training loss: 0.542296528816, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1871 : Training loss: 0.535976171494, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1881 : Training loss: 0.532775342464, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1891 : Training loss: 0.552302479744, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1901 : Training loss: 0.490476071835, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1911 : Training loss: 0.494087517262, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1921 : Training loss: 0.526910722256, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1931 : Training loss: 0.47756677866, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1941 : Training loss: 0.578582882881, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1951 : Training loss: 0.521679997444, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1961 : Training loss: 0.564016699791, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1971 : Training loss: 0.500248730183, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1981 : Training loss: 0.526161432266, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1991 : Training loss: 0.519133508205, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2001 : Training loss: 0.564579188824, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2011 : Training loss: 0.545312047005, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2021 : Training loss: 0.561658740044, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2031 : Training loss: 0.577349543571, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2041 : Training loss: 0.590047776699, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2051 : Training loss: 0.551764190197, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2061 : Training loss: 0.535576224327, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2071 : Training loss: 0.505968809128, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2081 : Training loss: 0.575042903423, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2091 : Training loss: 0.513619542122, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2101 : Training loss: 0.575933396816, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2111 : Training loss: 0.555475890636, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2121 : Training loss: 0.561696410179, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2131 : Training loss: 0.505744874477, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2141 : Training loss: 0.572377681732, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2151 : Training loss: 0.549990236759, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2161 : Training loss: 0.543768584728, \n",
      " test accuracy : 0.742857158184\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2171 : Training loss: 0.510122478008, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2181 : Training loss: 0.571035861969, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2191 : Training loss: 0.523146867752, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2201 : Training loss: 0.47478556633, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2211 : Training loss: 0.519850850105, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2221 : Training loss: 0.526078999043, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2231 : Training loss: 0.509840905666, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2241 : Training loss: 0.554421722889, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2251 : Training loss: 0.496166974306, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2261 : Training loss: 0.471783548594, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2271 : Training loss: 0.512223303318, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2281 : Training loss: 0.546135008335, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2291 : Training loss: 0.514976620674, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2301 : Training loss: 0.542277693748, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2311 : Training loss: 0.546191990376, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2321 : Training loss: 0.554865598679, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2331 : Training loss: 0.594774603844, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2341 : Training loss: 0.526831448078, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2351 : Training loss: 0.481724023819, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2361 : Training loss: 0.486029446125, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2371 : Training loss: 0.478176832199, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2381 : Training loss: 0.505101144314, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2391 : Training loss: 0.52789080143, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2401 : Training loss: 0.50192463398, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2411 : Training loss: 0.523658692837, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2421 : Training loss: 0.534842550755, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2431 : Training loss: 0.532964646816, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2441 : Training loss: 0.577601075172, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2451 : Training loss: 0.524571299553, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2461 : Training loss: 0.509110093117, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2471 : Training loss: 0.537798464298, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2481 : Training loss: 0.488337635994, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2491 : Training loss: 0.491653710604, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2501 : Training loss: 0.479673504829, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2511 : Training loss: 0.553980648518, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2521 : Training loss: 0.481101989746, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2531 : Training loss: 0.468523293734, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2541 : Training loss: 0.48714992404, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2551 : Training loss: 0.460674315691, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2561 : Training loss: 0.587208867073, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2571 : Training loss: 0.572462260723, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2581 : Training loss: 0.483604758978, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2591 : Training loss: 0.510521829128, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2601 : Training loss: 0.440119087696, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2611 : Training loss: 0.478420883417, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2621 : Training loss: 0.58814483881, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2631 : Training loss: 0.533826351166, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2641 : Training loss: 0.509681165218, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2651 : Training loss: 0.526006996632, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2661 : Training loss: 0.501695513725, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2671 : Training loss: 0.468851596117, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2681 : Training loss: 0.449474602938, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2691 : Training loss: 0.581986367702, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2701 : Training loss: 0.480466336012, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2711 : Training loss: 0.491822808981, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2721 : Training loss: 0.499019593, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2731 : Training loss: 0.472224414349, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2741 : Training loss: 0.544751405716, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2751 : Training loss: 0.489558935165, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2761 : Training loss: 0.559325635433, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2771 : Training loss: 0.555548369884, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2781 : Training loss: 0.591696321964, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2791 : Training loss: 0.507588267326, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2801 : Training loss: 0.507185161114, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2811 : Training loss: 0.546837449074, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2821 : Training loss: 0.559330821037, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2831 : Training loss: 0.536847174168, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2841 : Training loss: 0.545726895332, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2851 : Training loss: 0.547924458981, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2861 : Training loss: 0.554422557354, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2871 : Training loss: 0.567693769932, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2881 : Training loss: 0.522234737873, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2891 : Training loss: 0.526091516018, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2901 : Training loss: 0.515355825424, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2911 : Training loss: 0.513217031956, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2921 : Training loss: 0.513364732265, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2931 : Training loss: 0.533072113991, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2941 : Training loss: 0.5710490942, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2951 : Training loss: 0.464990288019, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2961 : Training loss: 0.521542131901, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2971 : Training loss: 0.518527388573, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2981 : Training loss: 0.523101687431, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2991 : Training loss: 0.5005890131, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 3001 : Training loss: 0.501680374146, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3011 : Training loss: 0.513128519058, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3021 : Training loss: 0.478024572134, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 3031 : Training loss: 0.549851000309, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3041 : Training loss: 0.556954264641, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 3051 : Training loss: 0.5197006464, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3061 : Training loss: 0.5042963624, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3071 : Training loss: 0.501670598984, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3081 : Training loss: 0.490654498339, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3091 : Training loss: 0.604350805283, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3101 : Training loss: 0.498879134655, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3111 : Training loss: 0.501317739487, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3121 : Training loss: 0.514781534672, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3131 : Training loss: 0.477768331766, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3141 : Training loss: 0.543476939201, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3151 : Training loss: 0.470493376255, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3161 : Training loss: 0.507150709629, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3171 : Training loss: 0.514678537846, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3181 : Training loss: 0.52651488781, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3191 : Training loss: 0.581639170647, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3201 : Training loss: 0.49303534627, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3211 : Training loss: 0.495196342468, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3221 : Training loss: 0.50249683857, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3231 : Training loss: 0.566352248192, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3241 : Training loss: 0.516731441021, \n",
      " test accuracy : 0.742857158184\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3251 : Training loss: 0.514011442661, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3261 : Training loss: 0.584486186504, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3271 : Training loss: 0.548297166824, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3281 : Training loss: 0.575819849968, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3291 : Training loss: 0.456868112087, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3301 : Training loss: 0.494415521622, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3311 : Training loss: 0.525944948196, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3321 : Training loss: 0.535638511181, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3331 : Training loss: 0.494950801134, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3341 : Training loss: 0.587163031101, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3351 : Training loss: 0.527218639851, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3361 : Training loss: 0.48192948103, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3371 : Training loss: 0.535349309444, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3381 : Training loss: 0.46899086237, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3391 : Training loss: 0.494821220636, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3401 : Training loss: 0.460323125124, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3411 : Training loss: 0.513280153275, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3421 : Training loss: 0.495665550232, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3431 : Training loss: 0.483116179705, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3441 : Training loss: 0.518746256828, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3451 : Training loss: 0.561976730824, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3461 : Training loss: 0.488931059837, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3471 : Training loss: 0.510203540325, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3481 : Training loss: 0.584822058678, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3491 : Training loss: 0.487810254097, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3501 : Training loss: 0.523265123367, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3511 : Training loss: 0.648016035557, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3521 : Training loss: 0.492384642363, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3531 : Training loss: 0.575856804848, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3541 : Training loss: 0.497077584267, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3551 : Training loss: 0.458353966475, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3561 : Training loss: 0.499393820763, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3571 : Training loss: 0.563467264175, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3581 : Training loss: 0.48785763979, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3591 : Training loss: 0.480992376804, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3601 : Training loss: 0.526147961617, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3611 : Training loss: 0.511688888073, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3621 : Training loss: 0.452837496996, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3631 : Training loss: 0.593787968159, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3641 : Training loss: 0.536925196648, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3651 : Training loss: 0.484618008137, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3661 : Training loss: 0.490397840738, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3671 : Training loss: 0.526319921017, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3681 : Training loss: 0.547043919563, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3691 : Training loss: 0.573248803616, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3701 : Training loss: 0.47625246644, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3711 : Training loss: 0.499269157648, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3721 : Training loss: 0.511767268181, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3731 : Training loss: 0.49638235569, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3741 : Training loss: 0.444677919149, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3751 : Training loss: 0.52003467083, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3761 : Training loss: 0.540527760983, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3771 : Training loss: 0.502104997635, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3781 : Training loss: 0.462162345648, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3791 : Training loss: 0.539299070835, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3801 : Training loss: 0.482870966196, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3811 : Training loss: 0.528644144535, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3821 : Training loss: 0.548328876495, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3831 : Training loss: 0.483098298311, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3841 : Training loss: 0.505565047264, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3851 : Training loss: 0.433571636677, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3861 : Training loss: 0.546049654484, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3871 : Training loss: 0.512234210968, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3881 : Training loss: 0.460798889399, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3891 : Training loss: 0.42758500576, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3901 : Training loss: 0.49493303895, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3911 : Training loss: 0.454209417105, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3921 : Training loss: 0.503635764122, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3931 : Training loss: 0.461740553379, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3941 : Training loss: 0.559523880482, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3951 : Training loss: 0.488261342049, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3961 : Training loss: 0.498856157064, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3971 : Training loss: 0.414782196283, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3981 : Training loss: 0.504857420921, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3991 : Training loss: 0.523014068604, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4001 : Training loss: 0.484077870846, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4011 : Training loss: 0.456723481417, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4021 : Training loss: 0.472963511944, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4031 : Training loss: 0.441414624453, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4041 : Training loss: 0.560329854488, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4051 : Training loss: 0.438546776772, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4061 : Training loss: 0.52443498373, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4071 : Training loss: 0.544520318508, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4081 : Training loss: 0.523515582085, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4091 : Training loss: 0.547391295433, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4101 : Training loss: 0.479822158813, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4111 : Training loss: 0.443333208561, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4121 : Training loss: 0.479195296764, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4131 : Training loss: 0.509914457798, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4141 : Training loss: 0.456952512264, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4151 : Training loss: 0.469113260508, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4161 : Training loss: 0.482463896275, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4171 : Training loss: 0.512117266655, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4181 : Training loss: 0.473822832108, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4191 : Training loss: 0.509525895119, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4201 : Training loss: 0.473033279181, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4211 : Training loss: 0.496237665415, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4221 : Training loss: 0.438475906849, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4231 : Training loss: 0.485097885132, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4241 : Training loss: 0.444409936666, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4251 : Training loss: 0.489711463451, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4261 : Training loss: 0.497537195683, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4271 : Training loss: 0.445626944304, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4281 : Training loss: 0.501701235771, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4291 : Training loss: 0.518128216267, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4301 : Training loss: 0.420329123735, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4311 : Training loss: 0.461684197187, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4321 : Training loss: 0.485362440348, \n",
      " test accuracy : 0.771428585052\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4331 : Training loss: 0.531469166279, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4341 : Training loss: 0.474008023739, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4351 : Training loss: 0.498501867056, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4361 : Training loss: 0.440185546875, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4371 : Training loss: 0.518564522266, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4381 : Training loss: 0.496960997581, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4391 : Training loss: 0.495551377535, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4401 : Training loss: 0.493839472532, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4411 : Training loss: 0.483566820621, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4421 : Training loss: 0.447363436222, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4431 : Training loss: 0.487783104181, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4441 : Training loss: 0.525516867638, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4451 : Training loss: 0.423942178488, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4461 : Training loss: 0.469454228878, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4471 : Training loss: 0.481012672186, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4481 : Training loss: 0.503388464451, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4491 : Training loss: 0.477403521538, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4501 : Training loss: 0.45612090826, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4511 : Training loss: 0.512575447559, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4521 : Training loss: 0.47960627079, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4531 : Training loss: 0.469964087009, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4541 : Training loss: 0.455540776253, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4551 : Training loss: 0.490922957659, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4561 : Training loss: 0.497162163258, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4571 : Training loss: 0.495264589787, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4581 : Training loss: 0.472310274839, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4591 : Training loss: 0.494942486286, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4601 : Training loss: 0.522546350956, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4611 : Training loss: 0.491932779551, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4621 : Training loss: 0.483473718166, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4631 : Training loss: 0.470119416714, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4641 : Training loss: 0.440945923328, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4651 : Training loss: 0.503126621246, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4661 : Training loss: 0.456955462694, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4671 : Training loss: 0.58563876152, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4681 : Training loss: 0.486101537943, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4691 : Training loss: 0.414012044668, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4701 : Training loss: 0.450306326151, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4711 : Training loss: 0.539581239223, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4721 : Training loss: 0.442197144032, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4731 : Training loss: 0.491660028696, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4741 : Training loss: 0.49211165309, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4751 : Training loss: 0.448205262423, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4761 : Training loss: 0.476861447096, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4771 : Training loss: 0.478534013033, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4781 : Training loss: 0.475015610456, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4791 : Training loss: 0.462872117758, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4801 : Training loss: 0.400776803493, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4811 : Training loss: 0.528398275375, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4821 : Training loss: 0.444914579391, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4831 : Training loss: 0.490899801254, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4841 : Training loss: 0.480942964554, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4851 : Training loss: 0.460425108671, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4861 : Training loss: 0.465108335018, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4871 : Training loss: 0.483148366213, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4881 : Training loss: 0.550799012184, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4891 : Training loss: 0.575493037701, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4901 : Training loss: 0.471054375172, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4911 : Training loss: 0.489831000566, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4921 : Training loss: 0.511512875557, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4931 : Training loss: 0.519863724709, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4941 : Training loss: 0.491863518953, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4951 : Training loss: 0.493174165487, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4961 : Training loss: 0.470610529184, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4971 : Training loss: 0.412418752909, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4981 : Training loss: 0.489101797342, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4991 : Training loss: 0.522384047508, \n",
      " test accuracy : 0.785714268684\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztnWeYHMW1sN/q3dUqreKQJBEECLAAk2yizSUHGwswuBAYGzAgJ4ENGAw8JhgwFuAPECZZZK4xogwYhI0RmHDBASyCSAKDEMESSatd5bjT9f3ont3ZmZ6ZnumenXTe55G2u7rCqe6eU9UVzlHWWgRBEITGwKm0AIIgCELfIUpfEAShgRClLwiC0ECI0hcEQWggROkLgiA0EKL0BUEQGghR+oIgCA2EKH1BEIQGQpS+IAhCA9FcaQECkC3CgiAIpaEKRahGpc/HH39cctpEIkF7e3uM0lQ/jVbnRqsvSJ0bhSh1HjVqVKh4MrwjCILQQIjSFwRBaCBE6QuCIDQQovQFQRAaCFH6giAIDYQofUEQhAZClL4gCEIDIUpfEGLCLu3EvvyvSoshCHkRpS8IMeFeezHuTb/Grl5VaVEEISei9ENgFy3EnfmnSoshVDvtn3l/XbeycghCHkTph8C9/lLs/XdgF31eVDr70TySpx2DXdxRJskEQRCKQ5R+GFat9P4W2YOzTz4Cq1dh33y5DEIJgiAUjyh9QRCEBkKUfhhUQWulgpCGWAcXqhdR+n2BFSXQGEjnQKh+ROmXE9EBDYY07kL1I0pfEGJHWnuhehGlX06k49egyIMXqhdR+sUgY/NCXqSHL1Q/daP07coVJE+dwGdH7hl/5qWu3hEdIAhClRHKMbrW+hBgKtAE3GqMmZJx/RpgX/90ILC+MWaYf+0E4Bf+tcuMMXfFIXgWya7uQ2stqizLLKWnXwp29UrsS/9E7bl/mZ6LIAhhKdjT11o3ATcAhwLjgWO11uPT4xhjzjDG7GiM2RH4LfCgn3YEcBGwG7ArcJHWeni8VfBQbUO7j91Jh2NlKKZqsL+/CXvndTDvP5UWRRAanjDDO7sCc40x84wxa4HpwOF54h8L3OsfHww8YYzpMMZ0Ak8Ah0QROB/OBdf2nCxfVoYSSuylNngDZJd0egdr11RWEEEQQin90cB/087n+2FZaK03BcYCTxWbNg7UJpvTss32ANjH7i9DCcUqbxnKEAShugg1pl8EE4H7jTHJYhJprScBkwCMMSQSiZIFcH45lU+O2Q/7+EMM2e9r9Nt6u5LzStHe1EQSGD58BM1FyLaktZXVQFtbGwMi1KkQzc3Nke5ZuelsaWEtMGToUFpjkLNa6/u5Ulhg5MiROIPaYs27WutcTqTOZSojRJwFwMZp52P8sCAmAj/OSLtPRtpnMhMZY6YB0/xT297eHkKsYNJvWOe5k3B+9xDK6f1BY9esgfnvo7bYJlSeyaTXhnV2dqBa+oeWxV2zGoBly5ezIkKdCpFIJIhyz8pNct06AJYuWYKKQc5qrW9qHmnRokWoVfEOZVVrncuJ1Lk4Ro0aFSpemOGdWcA4rfVYrXU/PMU+IzOS1nobYDiQ7i9uJnCQ1nq4P4F7kB9WVtRXD+o5CTBrbO+6DnfKOcXbuc8xuuM+/Sj2s4+Ly0sQBKECFFT6xpguYDKesn7LCzJvaq0v0VpPSIs6EZhujLFpaTuAS/EajlnAJX5YWXG+O7n72L3uEtyHft/ruv1wrnfg98SjYN0k9g83417+szyRGnsit+GQxy1UMaHG9I0xjwKPZoRdmHF+cY60twO3lyhfLNi/GDji+OgZ5ZuXTTlaCZ2gNkie8z3Ul7+K862TSs+kURq92n/cQgNQNztyM3EmX9Dr3Lou1lpsV1fpSqhBdFcvOtuxj4t/4FA04vsh1Bxxr96pHrbfpdep+/0jek7W2zBUFu5t12CffxrW36hEIUQLAI3nhKbBqivUFnXb01eOgzr06Eh52OefzggQJS4IQm1Tt0ofwPnmd4MvpDk4d594GPfmKwrkVGrXTbp8DYn0DYQqpn6Hd/JhPaXv/uIHYROUdEloMKSNF2qAuu7pA7DpltlhaT39ase6Sdy/PuBtKBOEPsTOmY3tWldpMYSYqXulrw4MsA1XtNLP04Urc+/OvvAs9sG7sDPuKW9BgpCGff9d3GsuxN5/Z6VFEWKm/pV+a2t24NLFWUF23Vps4Fr7KIXH0CKkNpCtXhU9r0rRKBPg9VTN5UsBsJ/lsrgi1Cp1r/T54q6oPfYrGM29/Gzc0ycWiFXkr7pRlJ0gCDVD3St95Tiok35SOOL89/NkIjN0kWiU+9cg1RRqm7pX+kB0F335euzSmRfqEnmx65WGUPoAbDm+cJwoBLUrjdLDFeoYeYfrjYZR+mrfr0VInP3iu/9+FnfaVT0BZe8YyY+vdpBeslC9NIzSL5bkjZdnB6b9lu0tv8HOeq7vBBJFEgk7dw7Jqy/wDO6VDWmYheqnYZS+ag3v8QqAV55PSxwuif3gXdxn/lpcOUKf4N5+Lbz1KnQsLGMpddQwy8qzuqVxzDB88culpw35/ru/Oss72OfQ0suqRxpOgdR+j9/66/SF+qNxevpKodI8asVGwym0WkeeVyHsR/Owd0yttBhCmWgYpR+JMB23snfuarj32HCrmGq7YbEff1RpEYQy0lBKX60fzlu87VwUb8HyNVBFlLMBarTGTahFGkvpb71dqHjuBT/KcaXITVpl7OHaNWtInjoB968PlK2M+kQa4KJouK+0cNjFHbhPPlJpMUqioZR+aNZkGjerzItvP3oPO+vZ4Isrl3txnqrNFy+F7WjHfeLhSosh9EIaxkK4N0/BTr8F+9nHlRalaEKt3tFaHwJMBZqAW40xUwLiaOBivDfmVWPMcX54Enjdj/aRMWZCDHKXjeSUc+LLLOKwjnvpGaHy7vpkPvbN2agdd49UXiVwb/gVfPQedqfdUYkNKi1OSVg3CXNmI8qygVjhdbpwk5WVowQKKn2tdRNwA3AgMB+YpbWeYYyZkxZnHHAesJcxplNrvX5aFquMMTvGLHf5eO/tIhNU+oeuWDT5WHCTNN0yo8Ky5CBf47dqhfe3hhzbZGKfeFjszjcclf7dl06Y4Z1dgbnGmHnGmLXAdCDTM8mpwA3GmE4AY8zn8YpZA/S57Z20l64Gext1xcJPKy2BUDFqb84jzPDOaOC/aefzgd0y4mwFoLX+B94Q0MXGmMf8a/211i8CXcAUY8xD0USuILIKRxCEGieuHbnNwDhgH2AM8KzWentjzGJgU2PMAq315sBTWuvXjTHvpSfWWk8CJgEYY0gkEqUL0tycN/1nJeTZ1NxMEhg+fDjNft6pfBIjE3ifNT0tfqr8pa2trAIGDx7MwBLqlC5r//79GZJIsO6DuaiWFtSIEbQDTpNDamAkyn0rJEOUvDv79WMtMGToUFoz8ml3nKx7W4jUM1439y2aNxuHai78Grc3NfnljAhdTliW9h9A+tT/yBEjcdqGdJ8nF3dgly2leePNSi6j0HsdhuV/mEbrnvvRslmA3+g0Vg1uo3s/7usvkjx1Auvd9SjOkGGRyi+WOOpcLtqd1Ps0LNb3qS/qHEbpLwA2Tjsf44elMx94wRizDnhfa/0OXiMwyxizAMAYM09r/QywE9BL6RtjpgHT/FPb3t5ebD26SSQSREkfRDLpDZ90dnaiBrT1uta+yC8r7SsgVb7rOzNfvnwZK3PIlDx1AuprGufI4/PKsHr1ata2t5M847sAOFfc5pXhZpdbDqLknVznOddeumQJKiOfpD+W39nZgWoJZx8pkUiw8PVXcC/8MeqACTjHnFJYBv8ZLvqRxrnpwVANRVjclEtLn0Udi1Br1vaU/cOjoGtdpDmXqO+1XbsG9493smLGdJquN3njusuXZYUten126CXPcVGO33JcdOuExYtR/eOTMUqdR40Ktw8pzJj+LGCc1nqs1rofMBHIfHsfwuvlo7VO4A33zNNaD9dat6aF7wXMQeiFfTT/jxDIHjps9JGmpUsAb1lr0aQmj+Mic9gv87xrXbzlRaGGJ8yrihoe6i2o9I0xXcBkYCbwlhdk3tRaX6K1Ti2/nAks0lrPAZ4GzjbGLAK+ALyotX7VD5+SvuqnLgh49u7MP2HfTavme/8pX/m1N49UEu6/n8UuqE7zAPb/HiscSRCqhFDfuMaYR4FHM8IuTDu2wJn+v/Q4/wS2jy5mjGw4Gj7NHJ2KAUV3A2DvvwMLqK8e5J3/80kI46c3DZvRk7DPPg7H59opXP/YW36DhXiWpZa7l1bru1gDd5f3uRTVTQ0/44bbketcciPO7x6CnfcMn+gTb/GSDVjDXzZHKm+/llFQDX+WF6Fkk1eei/vX+8soTINSu6MR1Uk9D+/UG0oplOPg7LFv8YnffAUAu7pnrYa949q4RPPyW7YEu+BDWLumUMxYy604K/xx9nfnYB+8O3y6Gv7xCfVA7fX4G07pR8FiscuW4p52TNnKcC8+Dffi04pIUXsvXRDu5WdhF8ds3TSdmD/H7XtvYz94N8fFGm+I6uOVEnIgSr9Yli4ODo9rVUSu/DNJKZYi3P+5Mx8keeoE7JpCXxExE1bhdnbEnrddtw733mk9tlKyrq/Frit+dY075ZweT2m1RJhHUeNtVt9QuzdJlH4DYf/mT4KuDFaA9Yh94RnsU3+GFelrz3t+sO6PjsY96zt9L5hQH9TghK4o/WKw3f9VLXb2C9h33qy0GCWSfW/tnFewiyKYcgrzBbZqZen5C0KN0TiO0WOhdIVv165B9WvtOe/qikOgLNwbfgWAc84U1LjxGUKUpciy4l5zEbT0yx2hGsfPyyyStRY6FqJGrl84cmyUt0drV67AvvIvnL0OKGs5gvT0S6C0l9/+8fZe5+7ZJ+KepovKw33qzz35PfdE/rhXnpv7Yhl+v/bVWaXtjg3DurXZYaHrUIWNQkTs4w/hnnuKt8qrryizJVd79/XYO6/LPTkuxIYo/aIpTYlk+d1dvhSK7O3be6f1HIcx3ZAzo9KT5sK9/tLcTl9C98aLaI2i1MFP6/7+xgiZVA77zhveQXsp5gPDFJB9c92nHw2IGGORSzq9g7UBDXw1Uo1fmCFpXKX/hR2KT1PsWHm6Ia5X/41dtjR33BrALluStVM4K84H73orhBaFX1WUlrr4JAETae7jD+H+pXCj2FjmEyIqqdWZLkTjpkaVaO3N4zau0let/VF77FdcouXFKW37797+bd0zj8cWscSybPgvqk2G/2S3Cz7EPfM72Odm5o/3rHfdvvlyyeJFxf7xduxDv88doUI/VDvnFez8DypTeDHU4IoUITwNq/QrRuYwT4Ww77+L+4MjsW+EVM4pUxRzZhdfWD4lElXBlPKZXfZOZXAB7jUX4f7y9HIXHkwND0dUN7XXQDa20i9F4URZPlhmCm8y6vnh27meFVD75svYzkUkz5+ELdcYcVg6i7Ajnnp0787Brsi2/94QlEuRV7KBkK+MstPYSr8E7GuzKi1CTtwfHVVSOvuvp2Dhp9hnKzvG7d40paR09p6bY5YkP8lrLiwcqeqokZ6+fJGUncZW+pXoVFTypU6tkMgkhEz2vx/EK0sRZQenSzuc9Rz24z60tV/KEFcA9j9vkJx0eOkT/AG9YtvR7k2k+8YBAxKVVlatNBpCQRpb6ZfyAyjTu5+89Iy+Xd0TqGxz349oS0TLrzDciybnKLvsRZeMO/NB7968H6OTnXme+W83c8I98n3omx6SffWFPimnkWlspV/S+GHEX0+uMj96D3fqxdHyDi9E7+OUUlYKu3wp7r3TsCW6+LPPPV6kKBX43IqxEbJBhtzqcYiijx6TffyhvikoKjX8jEXp9zHu9ZfmvuivkIkL6y8xtdZi5/X0Ju3jf+qJlExX7gp7/53Yp/6MnfX3WGWpGOV+xHmc29gVywL3Ndg1q0neeHlRFlKDC7DYdeuC3UhmFlvDSqqqqcGJ58ZW+pVgeZ6VJgUdpxSHe8bxuC/8H/bvT+D++uzucPvEwz3HTz/a47jbTULS3yWcS5m9WyUujvvwt2ZnP++tbipmX8Oihbg//TZ25oPZ117+F7zyPJRqRiFN0dh7bsS9eDJ2aWfWNfBt2gSZsRCiUcONqCj9Yumjh20XfhpPRv95vfAXhN/jtI89gH3+mfxxw9r7T6ePekPJy3+Wbf8nhsfl3n0DLPwUVizNVvy56uYv7bWvlne1l001wqsyd8x6FXd/cizuZWcS+UbUYI9WCCaUlU2t9SHAVKAJuNUYk7W2TmutgYvx3q5XjTHH+eEnAL/wo11mjLkrBrkrhn3pH9HzyOHQo1ec+26NXE40qvxHHqTD3n+nl/2f5FXnF7f2vxDtn+OedUJ8+ZWK3/Gwixb2NvWRizhWNtVwz1boTcGevta6CbgBOBQYDxyrtR6fEWcccB6wlzFmW+CnfvgI4CJgN2BX4CKt9fBYaxCF0ZsWn2blisjFuj89LnIeRRG3q8Cuddh33sAuXoT7h5uDhz0+WxBWuFhl68U7b3g99Jiwn8wPG7P0Mt54meS5p4QakrHTp/Usw41VKYuCr2fCDO/sCsw1xswzxqwFpgOHZ8Q5FbjBGNMJYIxJbVs9GHjCGNPhX3sCOCQe0aOj9jsM9d0cS/3qiaDJxPfe7jkp0tWjfeAu3KvOx/3VWd6cQMC69UKrMNyH78H90/8Wbc+oL7AfvYebNu/Rl7j33eoNDZW8OzpHIxpVj8vwTiSSp07AnXFvpcUAwg3vjAbSB4Xn4/Xc09kKQGv9D7whoIuNMY/lSDu6ZGljRikFW2xT9/0a+2lAr/vlf/Zcf+H/issvNQFZrD/f9KA/31dUmVlE0kH5n3j3ENGBmX2bUgQpw9sVpIAzw2pkOMZ2tMOQYTUjbxhsVxfMexu11Xa9wx+5FyYcWyGpeojLc1YzMA7YBxgDPKu13j5sYq31JGASgDGGRCJRuiDNzUWlt0OHUr3WdKJT9Lp5n7a2NgYkEgT1N/u1tLAWuhXNkCFtBKn/1HPoaGlhHaD+cDOJm/6Iu3IFhRYrptJmlp9IJHC6uhg6dCg59hcXZMSIETSN6F239HcmFT547ps0bTCKxY6DC7S1DSbzu2TkyBFZdRkxbDjJ1SvoBFqavbqnk5nPkCFDaPXLb29qIgkMHzac5jSZ0t/rzn79yBz8GT5sGM2JBKuHDGEJ0NqvlWFpz2/kCF9OpQr+PlYNbsuqZ7+WfgyP8LsMwl2xjIU//x4DDj6Srpae+5SSr9jfcl+y0H8nRowcSVOGjMvuvJ6VD/+BEb+5nZYttgF63qlC9emLOodR+guAjdPOx/hh6cwHXjDGrAPe11q/g9cILMBrCNLTPpNZgDFmGpDyEGLb20uffEskEkRJL3gsW7aM5QuDVfPabkcXntJfujR4iCb1HJK+Ibjkpwtob/fMBBQi1zP87CffgY/eQ+13WME8ctGxqAPl9u4ZB5W35IrzvIO2oQAsW549Ab9oUUd22F8fRI3dGoB1AY5ylk7tvVdj6dKlqNS98udHOjs7UAMGd8dJf6+TAY5GOjs7Ua0Dsf6zWLN2Ta86Lfq8p4kr9PtwA5YVr123rlc6d8a9qPE7oLYcnxU3LCnfx6tm/R1G9Ci6hQvmo1r7V/Vv2fWHRDs6OlBOS69rSX/odPFHH6KG9lbgheoTpc6jRo0KFS+M0p8FjNNaj8VT4hOBzJnIh4BjgTu01gm84Z55wHvA5WmTtwfhTfgK1U4M7vHssqWotiExCJOGvyTTvv5irNna1atQ/Qd0b2jrxbIluRMGje6EWJ0ViRKGQtx7bvIOwqz2CSPCI/diH7mXpltmxJJfrzp1dUFr7qhVQQ0PRxWcyDXGdAGTgZnAW16QeVNrfYnWOtVlmwks0lrPAZ4GzjbGLDLGdACX4jUcs4BL/DChyrF3/Tb3ix3yhXfPPN47qLZJwABx7J/vw65aiXvG8fGWNbfwZjb78UfZO3dLvWe5kmUYYHMfewD3rt+WVkYayVMnBE5624/meYbf/vt+8ZlW2euSl7zPqTobhlBj+saYR4FHM8IuTDu2wJn+v8y0twO3Z4YLNUAh+/wpD1yrVuaMYhd3wNuv9ZzH1QuO0JDYOa/i3nFt78BkF6yKvhy3JHkeuAv6D0Dt87WeXdn5GtYwdc9Mnr6Ld8Vyr0yAE04rTtigop55NGvS277yfPdftfHYHAlz1bGWtH4Q1S2/7MgVcmLvui7HhdSP1Xu57fRbcubhnn1i7/Obfh2u7BXLsHNymQeOhs1U+KnwfC4Wiyuh+CQfzfP+RnanGULhvPtG/uvFit/VRfLi07BvvFRkQqESiNIXcmJnPZfrSu/TfGPemYTctOVefDruNReFzzcG7L+eLiFVgJJ1K/FZn2VhLeO8jL3PJR2w4EPc/70xq3z7yL25PbJV27BfgyBKXyielMXOkswvh/yhLy7kS7gSCiPAaUkMZjnCYj+c6618mvtW8YnLebsKzPG4N19RxsIrhV/nKtxcWAhR+kLxBCxDDE2V9u5sqb6P33kzKLdIsuSi2xtWgE9g+1rmaqaM+7w60yBbkeR7bmn+GAIpdiVYlb4jQXjG7HJQpSt84tqcJQjhiNMAWpz4E495CXJP+Gm2PR770TxQRfanIioI+8fbcYcMw952dSokT+QCmQXp3NdfxM5/HzUmYFK2ZJeXOdLVjs4PpsobLenpC0IUPng3O+ytV7GPPVBcPgUU57oP3ysYp0fhA7NfwIboYbt/e5jk1RdkZJQj/7//LX9muZRdlSvBkqjOTnwopKcvCFWOfXcOHVee270zOCzuj3XhvO+7LXyGRTiRKYqOhdDaPy2gDhuJKkJ6+kJtUqE19X2N7erCLvzEOylmlRSUONGeh2KssRY75NPL0U8NdKOrdLw+DKL0AeeqOyotglAsxSrAWsOCnf8B7g+/2b3RKb68S1RYhYaL6nEYJwRBfpC9CwFBbjLYp3EfIkofUMNGVloEoeEJMD/94VzvYPYLFZfFCy7O70IPARPgH83DnfrLEvOrAtIbuCLsQNk/3+f5NJ5fgnmKmBClLwhVgP3Hk70DFOXrOZeab6EPhIWfevZ2Fn5acCjIvft6CFj5FKqcCpG87Ezch+/xTtJ69/nMkGRi33/HO+isnAkyUfqCEJaoa92LwH3o99BRpuWtBYd3ghsF+9ar2MWLsAXmCtzzJ/V2UF+jwz7J8yfhpq+I+nBusPMfp7bUqKzeSTFyfc9NnSDkwP7h5r4r7JXn4x/Lj0pnO+7ZJ6F237dw3DcL2E1a8EHua300SWqXLQFUbvPfCz/1vlpOzrMBC8i32si+/lLV2SQSpS8IVUKgg/mKUGA/wKsxzDFE2dUdE+6Z3wEo0SdAzz3K/SFjca/LmLeoglU/tfVdUk56rRMWhAoQs2OYslEFiquc2I8/8nr4ua5nNFglrcap4IiXKH0f57QLCkcShHLSV8MaH0dcMlj2uY3KNiruRZNxz5+U87r9+xO9RLR/nh4cMe59EjEhSt9HJTZAHTax0mIIQtmxqRUouZj/QZ/IkZO0CWz7+SfeiqAAa6b2vbexH77XO8xaz5vX/95QPvlCKnP3j9W5/0eUfhrqG6L0hcph33i50iJg167B/i0mv7cl4v7ydGxqyeeCD72wAF8H7pRzcC87A/exB7q/XuwfPSd99tmZ5RNw/vv5h2dSg/xVujBElH4aqsaWXgn1hX32sUqLUBUTrB7++ElLi/c3j+tO+8BduL8+21tOGuCvN3bJ/vFkpUegIiFaThCE8lLSOn2V8bcA69bC8mw/A2Fx//awN4xUog9n98G7s62VBlEFjYUofUEQqpAM7fjhXGzKaXw5Snv2ce9gcdidsr3ls3+9H956tagykzde7nlC62NCrdPXWh8CTAWagFuNMVMyrp8IXAWkHKBeb4y51b+WBF73wz8yxvR9LQVBqBwRduS6907zDlYsw73lNzhHnYjacHS4tE8+grP/N0ouOx27LMMt4prVpWWUfiv8zXfJUyeUuFegNAoqfa11E3ADcCAwH5iltZ5hjJmTEfU+Y8zkgCxWGWN2jC6qIAgNQ6oj/fnHPWGzX8Cd/QLOzX9CNTXlSeSfTb8FQih927UOcjlvh+DeeL5lq3ldSxYUp+yEGd7ZFZhrjJlnjFkLTAcOL69YgiAIOShxP4Pt6sKddhX2k96G3twfHuXNCfQFc1LmKSq3OyvM8M5oIN3DwXxgt4B4R2mt9wbeAc4wxqTS9Ndavwh0AVOMMQ9lJtRaTwImARhjSCQSRVShN83NzZHS527vBaF+Sf1m3BX9WRhz3k2rV5JIJLCrV5HsaKd51MYFf2eJxEiam4PV09COz+g3foesPIYNH0HmiHy6Llj79ut0znqO5iUdjLjilkAZhg8fxqJCFcord4LOfv0o1IQMGTqExQFyRtVfYYjL9s4jwL3GmDVa6+8DdwH7+dc2NcYs0FpvDjyltX7dGNNrR4UxZhrgD95h29tLty6YSCSIkl4QGpHUb8aujN8jWfKT+bS3t5O85kKYMxvnd1n9vmx5Fraz3oYbBl7rNHfQNPkXWeGLOzuz80nTBclf/xyAdV1dLJwZPIbe2bk4MDws7e3tJNcW/mpYuqT3HEFKzij6a9SoUaHihRneWQBsnHY+hp4JWwCMMYuMMamp9VuBXdKuLfD/zgOeAXYKJZkgCPXFnNkA2Kg7VV/9NzZzOCaMsbqlvkJfuQL35in54/Yx7l9Mn5UVRunPAsZprcdqrfsBE4FezaTWeqO00wnAW374cK11q3+cAPYCMieABUFoIOxTj0TOw/3R0dmBq0M6M8nlvKWC2Id+32dlFRzeMcZ0aa0nAzPxlmzebox5U2t9CfCiMWYGcLrWegLeuH0HcKKf/AvA77TWLl4DMyVg1Y8gCI1EGAfrb78GOYZ3cmZ7U3X13vNhK2hRNdSYvjHmUeDRjLAL047PA84LSPdPYPuIMvYpao99sQF2PgShIaiSXrDNs4QyJ0ujjcf3JfapP1esbNmRm4E66aeoA2RFqtCY2CJ3lYal2J2n9pV/lUUOQZR+FkqpmvN5KQixUS0OUubMxq6JbnYh+fPv4f71/hgEqh9EwLxRAAAcV0lEQVREuwmCAIBdsxr7eOHllH3F4svPjp5JRzv2wbuj51MMZXcyEw3xkRvEwEGVlkAQ+hT3bw9j77ut0mL0Yu1r8U12BtnjD8SGmGQuxH9eLxyngkhPPwB18JGVFkEQ+pRqU/hxY59/Jly8WvFTHAFR+gGo5hbU3gdXWgxBEOKi2+ZNASKab7bVMieSB1H6giAIPvaxB6NlEMHpvPvX+3vcRJYRUfqCIAgpQjo9z4Wd/ULpaR+8mzX/fi5S+WEQpS8IghAXURuNsKYkIiBKPyeVs3ctCEKNEnVMP4zhuIiI0s+F6HxBEIolos63ya545MiDKP0cqH0OrbQIgiDUHLJ6p2ZRY8bC1jVlK04QhEojSzZrG+eHWYZDUUefVAFJBEGoBWxEOz9dH7xXOFJEROnnQQ0anBXmHHwk9OtXAWkEQah3Vv31gbKXIUq/AOrkM4NC+1wOQRCEOBClXwBn9316jn9wrn9U/eN2giAIQYjSLwK1y56VFkEQBCESovRLQoZ3BEGoTUTpl4DaafdKiyAIglASoZyoaK0PAaYCTcCtxpgpGddPBK4CFvhB1xtjbvWvnQD8wg+/zBhzVwxyVxR14unYF/6v0mIIgiAUTUGlr7VuAm4ADgTmA7O01jOMMXMyot5njJmckXYEcBHwJbzZz5f8tJ2xSF8hVHNLpUUQBEEoiTDDO7sCc40x84wxa4HpwOEh8z8YeMIY0+Er+ieAQ0oTtcIMbqu0BIIgCJEJM7wzGvhv2vl8YLeAeEdprfcG3gHOMMb8N0fa0SXKWjGc758Dm42rtBjZOA70gdMFQRDqh7gcoz8C3GuMWaO1/j5wF7Bf2MRa60nAJABjDIlEomRBmpubI6UP5JAjsoI+yyx3sy3p+mBuVrwBBx/Jqpl/ypu9MyKB29FetFhN629E8tMFhSMKglAzxK6/Mgij9BcAG6edj6FnwhYAY8yitNNbgSvT0u6TkfaZzAKMMdOAaf6pbW8vXgGmSCQSRElfKu5Zv8J5/x3cqy/oFb56yPCek36tgT44Xbe0zV7JPrC9LQhC31Kq/ho1alSoeGGU/ixgnNZ6LJ4Snwgclx5Ba72RMeYT/3QC8JZ/PBO4XGud0nwHAdlWzOoA1X8AfGGH/JFyDsXIDl9BEPqGghO5xpguYDKeAn/LCzJvaq0v0VpP8KOdrrV+U2v9KnA6cKKftgO4FK/hmAVc4ofVP5ts7v1VaRu5ho8sPp8ddoUR68UjkyAIDY+y1Wf/2X788cclJ+6r4Z3kqRN6TtbbkKbLp/UKV/t/A/vkIyh9Mtbc5oVNOgc77cqsvJyLf4t97nHsk49kXVOTzkZtuzPuT47NFmK9DWHhpzHURhCEaqHplhklpfOHdwqaC5AduXGQ6tUHkfYInC9/JTjK6E1xJp6aO4uBgwLDHX1yKPEEQRBSiNKPg4CPJXXo0bDdLqg99w9Os9V2kYtVO+6W3SsYtQnq6zpy3vWCOuknlRZBEKoKUfqxkK311dDhNP3kItTAbEcsAM53flQWSdTu+xY9B+Cc8cuyyNInbDm+/GWM36n8ZQhCHyFKPwac/b+RP0LQbt4iTTmok89E7bJX4Xhbb0fxq4Fq12qoGrVJoRgxFBI9C0GoFuLanNWwhJl0ca66M1sPq+I0ibP7PrgrlsNL/ygcuerm5stJH1RWSd9IqB/kbe4DVHMLqsXv2W+xjfe3X2v5CrRFmmZQCueMS2DsVvGUP3REPPnkwfnhuaiMyW+179fLU1iRDbQgVDOi9PsY54xLcM69EtU2tITUIXq1ShVvj6dfK2r8jqjEBiXIFEBrGRs0H7XzntnDagHDaCqqLGM2wzni29HyEIQqQpR+qWw5HrXr/xSdTLX2R6V6++Uih9J3fnY5fPHL2Rc23xoANSFgL0AJqE23jCWfkKXlv9x/YKTcmy66DsZsFikPQagmZEy/RJp+PqVwpAI4194DXV0xSJNBjnX9auvtcJqbcV+b1RO44+6o1PBFjnSZOBdcg3vpGbkjDBkWXP7B38TOfDBUGbERx+ZDGdMX6gh5myuIGtSGGppmkG3M2PCJ+w/Ifa2lXxFCpB0PLOwzQB1xPGqTLcLnn4Zz9Iml7TaMMkcwIFpPH+hpFAWhDhClX0U451+Fc9303BH8Xqva7zCafntfaYWkFJjjPXrlD+0AqObmYKU8rmctvCq0PLXKKPtQmiDUGKL0qwjV0g8VuWeao1c6IGPoZtMtcS65AXXQkQVzbDpnCs7V/4sz+QLPmiigTv0ZjN0K56LrshMEDKk4P7u8YDk5GbVx4TghCFNXKN32iSDUAjKmX0uEGWZoG4LqbO+1zse57GYI2BmsNgqvTFXbUNihZxLY2XVv2HVv72TgYFi5PK+c3qYxn/U3gs8/yYqTC0efjPvL00PHz5nPt07CXX8j7O9vzBlHfet7kcsRhGpGlH7cbLIF6n/K5Aa4wKSkc8kNqMQG2BEJ1DcmYh/xhorUBmnOFdbbyAv76kHlkXGbL/ZS+s5F18G6db3lvPi3kEzinnZMuDzD7l72b4/aa3/sP56EEQEeiArkpXbZM1xZglCjyPBOzDRdcA3O3gdXpOxUz105TTgTjguO0zaEpltm4JRJ6TsHHN698UztfTBqzGaosb39C6uWft3DRN1h+Za/KtXL/k3//b6Wdi0gvs1zMd3q6fm/wZn2cO/ruewWNTWhDg++p4JQS0hPX8hmUBusWFZ0Mufae1CD2rDz3vYChhfh63Oj0bmvDR2Oc9oF0LUWmpoZssGGLOrI44unW7EHfRn1aH1VxA7kpps9P8fJh/9QMK5zxi9xr7kodN6C0JdIT78WKfcSwqLzzzHsVCAf56KpBbNgUBuq/wBUczOq/0DvK8FJe203r77VOUqscgpVjCj9KsSZcmvwztlB/jr6wUP6VqAyodL3JZTYjqk99s0ObPVXGOXzWZC2VFWIyPobVVoCoQhkeKcKUSPXR/VrzTbMudv/gJtE7bZPmSWIaJq5mF2wTU2QTKL2PQwbYugkq+Sgr4lBg3F+eX33pHVguhyKKuxGLHXABOzfZGmnUHtIT7+GUI6Ds9cBqOZqa6tjMHXQ2j84vCTDdJ6d/W7Lpr0uZCt151c3F52/c8wp4eQ46Aic603vwO12LiiTIJQLUfpCAKUqoXiVlzrxdM/kc5nlUOuPKhwpiMFtUKABVkedgMpo0NROe/Sc9GvNb1IjboaNxPl/d/ddeULVEarLqLU+BJgKNAG3GmMCrY1prY8C7ge+bIx5UWu9GfAW8B8/yvPGmB9EllooM0X23AcOhpUrStP5Q4ZDZ3vgJWevA0rIMIBNtsguIwZDbM5VdwEW94dH5YyjnKaAhD19LedXv8O9sDyuM7PYfGuazrsq+FpzC3StC75WiKA6ClVLQaWvtW4CbgAOBOYDs7TWM4wxczLitQE/AV7IyOI9Y8yOMckr9CVNTaivHFgwmnPmpdjXXszpDzhv2p9Pwf7n9bIOWTVdcE3RadTXNPat2fnjxCCzGjaC9NbSmXwB7vWXlp6h4+Q0rZ3X3eZ2u8Ds50src/hI+HR+aWmFPifM8M6uwFxjzDxjzFpgOnB4QLxLgSuA1THKJ1QQ54rbcY4v3AtV622Is/9haSHhe9Fq5Po4e+5fgnQBDPetcQ6L7rnLOfJ4ms7/TeR8Ask3hl+K7aX0OYLWAbBB9p4H5/o/og4M+tn6IonPgIYhTFdlNPDftPP5wG7pEbTWOwMbG2P+orU+OyP9WK31K8BS4BfGmOcyC9BaTwImARhjSCSK2NSTQXNzc6T01cLi1lbWAEPa2uhfoD6f+X8z650rvBCfKwcLjBg5kqYSFOiyAQNZCQwaNIhBRZSd/N0D0NKP9u95ljxzyZ35jFP1XO+I41gzamNad/1q77X8aaxqa2Mp0NraytCAPArdq6B4n2XESSQSfAaogYO749npT/H5xP0AGPnV/Wm/87ruuJ8rhQUGfesk+g0bSmdeCbLp19zMWv9YOQ6O45DMiLPe6N4NQabMAwcMYEWBcgYefhwrA1ZY9Wtp6S6/FJrHjqPr/Xcj5FBflFt/Rf4+1Vo7wNXAiQGXPwE2McYs0lrvAjyktd7WGLM0PZIxZhowzT+17e3BY7xhSCQSRElfLbhr1gCwdNkyloesT656F3s/rD/e3dGxCNVVpOtFwF21EoAVK1awqpiynRZI9nwl5JI78xk7F02FRe3eLt0tt2V5nt267nJvp/Ga1WsC8w97r/LFa29vx7ngWhg2vFe8lPXOTrd33NT9XvWVA1lVglOdtWvTVO5hx5B8+i9FyQuwctWqguWsyjF8t3ZdyLmA1gGwJrucrmEjAVH6KUrVX6NGhVuQEGZ4ZwGQbo5xjB+Wog3YDnhGa/0BsDswQ2v9JWPMGmPMIgBjzEvAe0BM3rfrG/V1DettiBpfiemQGJZgQp8tRVRjxqJ2CNjMFhw7UlnOeVfhXJrbSmd3KZtsjhoyvGC8rHQ5vI7lpV/P6iA1fsd4vIUB7LxHr1O1z9dyRAyH2vWrkdIL8RCmpz8LGKe1Houn7CcC3ZanjDFLgO7vEa31M8DP/NU76wEdxpik1npzYBwwL0b56xY1ZjOaLp9WOGJ5pSgtWUw6pxyowW2eeEEWOMOkL+dO3hLum/Or32Ef+n3vpKUo/SDbdNvujH35Xz3njgP9+sHaEIM5G4yGz3r6hupbJ6H2n4B97vHiZasmijQLXo0U7OkbY7qAycBMvOWXxhjzptb6Eq31hALJ9wZe01rPxlvK+QNjTB5LWUJdUY2bjrbdGecH51aXxcwSb5Nz5R3ZO4uVKk3p59orkDmkEyJr5+dTUAdnOKwZMAjVVNtLO9VeB3i74mucUGP6xphHgUczwi7MEXeftOMHgAciyCdUkmpU2hFRSkGV2cxXex+CfeyBbpPUodMNHxkY7hxzCu6Nvqey5mac35rAeL3Iocydsy7FvfSMnjKP/yH2/jth2ZLccm05HvtJxhLOuIacwjJmM5j/QaxZqm99DxswX1JryI5cQYhKYoNIydU3v4tz04PZZiNy2fYvlN9Ou+P84FzvZPsvRdhPYFGbbNErxNlzf5qu/t/ishnUhtpp97xR1Hcnew54qolhGY1qnfSBROkLQkSck88oHCkPSqlgxbyxZ4VU7ft1nJF5GoA4etH5nNFsumXh5Id/u/f5l3smbZ3LbvLcbeZMrHC+ehDOpMzV3v7l/b9RsPyykGW7qT60vih9IQ+lTuRW8UxuLqKYB05Vd+xWODfeH4s4AM4pZ+Gc/Wuc476PE7bXn3Jck3p0UR6Fv1HMOfNSz8VlHpzDeru+VP0HBPplLgW13S49J5uNyx0xnYB3UO0eYIa7mDyUoqpXKYRElL5QRmqnZ+T8/Aqcn/0qYiYOqqVfPALhKU611baFI/qTsM4Pz0tzQxnt3qtvfre7t64GDkKN3jRCZiFlCTPRW+KQFwD9vaWt6rjvh4ufKXftvM55EaVfL4wbX2kJaho1ZBhq6+1LTByvLEEMnpjblLP61vdQRxwPO+6WM06xqL0OCO1boCBBve5jTs4Oi+nLoLAcdaK9S0SUfh0w8ob7cE6vHp+sag/P3IDaZY8CMYWwtO6ceyJUDRyE83Wdw/REBYcjNvPnAgLmK5wDctsByqJX4xOuPmrfr4fPP2bUt6vbkLAo/TqgedTGaZ/1lUeN3oSmW2aUbqe+1vB33qotq+hrK2ovvUD69InaXHM4zg/PxTn3SlT/EozIpVPCHFHms8icaC5dljzXttoO58o7cCLsXG7erPCkeVSqzQWTINQcav2NPNMMedwzZqU5dhJqwzEllef88Dzs2pDGbHMpzC22QY3dqmSXj+qUM1G77o17Q+55ENV/IGxR2HG9Gr1ZSTLkzzTjvF98cy25aDr78sh5tIwbH8l4XRhE6QvZ1P4ChT6nWAXu7HdY4Ui5ytp5j8Kj0s3eckPVGvwF2HTulQDYAw+HZBL7yr96RyiwUUw5TdgiN5N1M2oT769vqqHXV8PW28N/Xs8orISvljje4Q1Gw8JPAy+pg47APv5QDIVkZlz++QYZ3hGEauMLO0TPY9udUEccX3ClihqxHmq9DbvNOajjf4Rz7T1ZLh7zZxJeUTk3PYBz4dSM9OGLKopeG8JU93JWNWRoXt8CKZyjT+o52WKbXobt6Nfaszw2VkTpC5WgsRc3VBzn9Atxpt4bKQ/lON7k7qBwK2LUjrvjnP//UHsfjBrUVlxhRYy5q+aWvrHBo6DpR+ejDugxD6YOOcrbqbzznjj65G5T1wDqsInZebQ0w3obAt6XkXKcXnV1Lri2NNlyfH31FaL065nWAaHcHQrVhWpuQQ0c1Pfljh1X3DLNzLj5dt0GJd92J++gnEs1B/j3sf8AVFMTapc9A+uott8lKwzAufQmnJseDLym2ob0HH/pK72vpX8lZGWapnZ32DVwdVM5kTH9Oqbp+vsqLYLQQDiX3AC+k5owKH0y6qAjivIh4Oy5P+7LGfMP238JXn8xuIxDj4aBg1BfOSB0Gb3Sl/hV4hx8JMn77/Dy2P8b2CcfyVGAQ19/WovSF7Kp8ESuc8Xt0FzbZngbETV4CAweUjhiKn5TE4xcPyMwjwL8wg6oHXbNDs/TU1YtLb2GeDJxzr0S+8l/c17PTRHzGBNPJZlL6TsZph1kIldoRNSIRElep4T6w7nitvz6tQQfzumoLbbB+cqBOeYlyqWAe8pS28QwaV8kovSFbGQiVwjDJlt4FjIPPTq2LJ1jTumxLjpwECqPrR3n8mk4F18f75fp0BHePyAoY7X/N2Cn3VH7l77kNr2BUfscmlFA6dmGRZS+kI2s0xdCoAYNpmnaw7H6cVZjNsP5xdUwcBBtp5zpB/pqKrX34IAJMGykt9Q05Oqk0IxI4Jx1mafcAzbbqUGDvVVB/jCWOuUs70IxS1zT81MK9aW+9R0sY/qCIFQVymmiaeq9DEgkWNHeDltvjzr0aNQBnl1955hT4JgeA3Rqu52xs5+Pr/yNxqAmnhou7pe/Cu2fFWfrJ9Ni8wmnwXobYh+Jtkw3LNLTFwShqlGOg/PN7+ac51F7H1wxRyvd+yEKLLFVB0zoWdI6epPe15qboS38BHhUpKcv5EbG9oUaQCmFzdxQVmX+nVNfJ/atV2GTzXF/+u3ujV+9Kb/coZS+1voQYCrQBNxqjJmSI95RwP3Al40xL/ph5wEnA0ngdGPMzDgEF/oAGduvKpxLboQB1WNNtaop1XtbmRsL5ZvYcK434PQsS1Yj18cCzaM3KbvBtYLDO1rrJuAG4FBgPHCs1jrLhqzWug34CfBCWth4YCKwLXAIcKOfnyAIRaI2GoPKdNYteFRXx74gqrU/Ks0Hr/ril3HOmcKAr3+r7GWHGdPfFZhrjJlnjFkLTAeCrBVdClwBpNt8PRyYboxZY4x5H5jr5yfUAjX2QxKEbortsa+3gZdsz/3LIEw41Ljx8Xkry0MYpT8aSN+yNt8P60ZrvTOwsTHmL8WmFQRBiErUZY9qyHCabpmB8z+HxCQRng3/bb4YX34xEXkiV2vtAFcDJ0bIYxIwCcAYQyJRusnS5ubmSOlrkbjr/LlSWGDkiJE4Q4ozotUXyDNuDIqqcyIBf/onHed9n3Vvv87wkQmaK32/7num6CR98ZzDKP0FwMZp52P8sBRtwHbAM1prgA2BGVrrCSHSAmCMmQZM809te3t7WPmzSCQSRElfi8RdZ+tPgi3qWIRauy62fONCnnFjUEqd7Sk/Q734Dxb3GwA1eL+iPOdRo8K5Jw2j9GcB47TWY/EU9kTguNRFY8wSoLtp0lo/A/zMGPOi1noV8Aet9dXAKGAc8O+QdRAEQSgKNXR4NBMJDUDBMX1jTBcwGZgJvOUFmTe11pf4vfl8ad8EDDAHeAz4sTEmGV1sQRAEoRSULXU9a/mwH3/8ccmJ5TM4Osmffw862nGu+X23jZFqQp5xYyB1Lg5/eKew++SSchfqGufMy7Av/7MqFb4gCNEQpS9koTYY5XkcEgSh7hCDa4IgCA2EKH1BEIQGQpS+IAhCAyFKXxAEoYEQpS8IgtBAiNIXBEFoIETpC4IgNBCi9AVBEBqIqjTDUGkBBEEQapSCZhiqsaevovzTWr8UNY9a+9dodW60+kqdG+dfDHUuSDUqfUEQBKFMiNIXBEFoIOpR6U8rHKXuaLQ6N1p9QercKJS9ztU4kSsIgiCUiXrs6QuCIAg5qBt7+lrrQ4CpQBNwqzFmSoVFKhmt9e3AYcDnxpjt/LARwH3AZsAHgDbGdGqtFV69vwasBE40xrzspzkB+IWf7WXGmLv6sh7FoLXeGLgb2ABv2e40Y8zUeq631ro/8CzQivdbvN8Yc5Hvj3o6MBJ4CfiOMWat1roV7x7tAiwCjjHGfODndR5wMpAETjfGzOzr+oRFa90EvAgsMMYc1gD1/QBYhidrlzHmS5V8r+uip++/RDcAhwLjgWO11uMrK1Uk7gQOyQg7F3jSGDMOeNI/B6/O4/x/k4CboLuRuAjYDdgVuEhrPbzskpdOF3CWMWY8sDvwY/8Z1nO91wD7GWN2AHYEDtFa7w5cAVxjjNkS6MRTbvh/O/3wa/x4+PdpIrAt3ntzo/+bqFZ+gudvO0W91xdgX2PMjsaYL/nnFXuv60Lp492EucaYecaYtXi9hsMrLFPJGGOeBToygg8HUi37XcARaeF3G2OsMeZ5YJjWeiPgYOAJY0yHMaYTeILshqRqMMZ8kurRGGOW4SmF0dRxvX3Zl/unLf4/C+wH3O+HZ9Y5dS/uB/b3e4aHA9ONMWuMMe8Dc/F+E1WH1noM8HXgVv9cUcf1zUPF3ut6Ufqjgf+mnc/3w+qJDYwxn/jHn+INg0DuutfsPdFabwbsBLxAnddba92ktZ4NfI73Q34PWGyM6fKjpMvfXTf/+hK8IZFaqvO1wDmA65+PpL7rC15D/rjW+iWt9SQ/rGLvdb0o/YbCGGOpU3MVWuvBwAPAT40xS9Ov1WO9jTFJY8yOwBi83uo2FRapbGitU/NUL1Valj7mK8aYnfGGbn6std47/WJfv9f1ovQXABunnY/xw+qJz/zPPPy/n/vhuepec/dEa92Cp/DvMcY86AfXfb0BjDGLgaeBPfA+6VOLLNLl766bf30o3gRnrdR5L2CCP7E5HW9YZyr1W18AjDEL/L+fA3/Ca9wr9l7Xi9KfBYzTWo/VWvfDm+SZUWGZ4mYGcIJ/fALwcFr4d7XWyp8EXOJ/Ns4EDtJaD/cnfA7yw6oSf6z2NuAtY8zVaZfqtt5a6/W01sP84wHAgXhzGU8DR/vRMuucuhdHA0/5vcQZwEStdau/EmYc8O++qUV4jDHnGWPGGGM2w/uNPmWM+TZ1Wl8ArfUgrXVb6hjvfXyDCr7XdaH0/fG+yXg34S0vyLxZWalKR2t9L/AvYGut9Xyt9cnAFOBArfW7wAH+OcCjwDy8yaxbgB8BGGM6gEvxGsRZwCV+WLWyF/AdYD+t9Wz/39eo73pvBDyttX4NT9YnjDF/Bn4OnKm1nos3hn2bH/82YKQffib+ig//XTfAHOAx4MfGmGSf1iQa9VzfDYC/a61fxWuY/mKMeYwKvteyI1cQBKGBqIueviAIghAOUfqCIAgNhCh9QRCEBkKUviAIQgMhSl8QBKGBEKUvCILQQIjSFwRBaCBE6QuCIDQQ/x/DlhqGkLBntAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e19054090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "mlp.train(X=X_train_scaled, y=y_train, X_test=X_test_scaled, y_test=y_test, num_epochs=5000, lr=0.0001, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mlp.predict(X=scale_X.transform(test_split_list[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results[0][:,0] == test_split_list[0][1][:,0]).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
