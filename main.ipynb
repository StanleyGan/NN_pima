{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Pima Indian dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import data\n",
    "from model import MLP\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "seed=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "#TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve imputed train and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentPath = os.getcwd()\n",
    "test_list = data.testImputed(currentPath=currentPath)\n",
    "train = data.trainImputed(currentPath=currentPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Outcome into two columns instead of 1: Diabetic and Not_Diabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For train\n",
    "train[\"Not Diabetic\"] = (train[\"Outcome\"] == 0).astype(int)\n",
    "train.rename(columns={\"Outcome\": \"Diabetic\"}, inplace=True)\n",
    "\n",
    "#For test\n",
    "for i in range(len(test_list)):\n",
    "    test_list[i][\"Not Diabetic\"] = (test_list[i][\"Outcome\"] == 0).astype(int)\n",
    "    test_list[i].rename(columns={\"Outcome\": \"Diabetic\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into target and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.as_matrix()[:,0:-2]\n",
    "y_train = train.as_matrix()[:, -2:]\n",
    "test_split_list = list()\n",
    "\n",
    "#First element is X, second element is y\n",
    "for i in range(len(test_list)):\n",
    "    test_split_list.append( [ test_list[i].as_matrix()[:,0:-2],\n",
    "                         test_list[i].as_matrix()[:,-2:] ] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_split_list[0][0]\n",
    "y_test = test_split_list[0][1]\n",
    "scale_X = StandardScaler()\n",
    "scale_X.fit(X_train)\n",
    "X_train_scaled = scale_X.transform(X_train)\n",
    "X_test_scaled = scale_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate and train a Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = X_train_scaled.shape[1]\n",
    "outputDim = y_train.shape[1]\n",
    "mlp = MLP()\n",
    "mlp.buildModel(neurons=[10,10,10,10], activations=[\"relu\", \"relu\", \"relu\", \"relu\"], \n",
    "               dropout=[0.5,0.5,0.5,0.5], inputDim=inputDim, outputDim=outputDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training loss: 0.70648676157, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 11 : Training loss: 0.702401518822, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 21 : Training loss: 0.70127260685, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 31 : Training loss: 0.703016161919, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 41 : Training loss: 0.69585609436, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 51 : Training loss: 0.695337116718, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 61 : Training loss: 0.685764968395, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 71 : Training loss: 0.682702302933, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 81 : Training loss: 0.686173856258, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 91 : Training loss: 0.68150472641, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 101 : Training loss: 0.672257125378, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 111 : Training loss: 0.668359220028, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 121 : Training loss: 0.652190566063, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 131 : Training loss: 0.654684960842, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 141 : Training loss: 0.639796972275, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 151 : Training loss: 0.66395586729, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 161 : Training loss: 0.629236578941, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 171 : Training loss: 0.651956439018, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 181 : Training loss: 0.637801587582, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 191 : Training loss: 0.666927814484, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 201 : Training loss: 0.634997963905, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 211 : Training loss: 0.63240468502, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 221 : Training loss: 0.65698581934, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 231 : Training loss: 0.600256979465, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 241 : Training loss: 0.582064807415, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 251 : Training loss: 0.632732450962, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 261 : Training loss: 0.610587060452, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 271 : Training loss: 0.593069732189, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 281 : Training loss: 0.582041501999, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 291 : Training loss: 0.614672780037, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 301 : Training loss: 0.623591780663, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 311 : Training loss: 0.589031875134, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 321 : Training loss: 0.53081035614, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 331 : Training loss: 0.604298353195, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 341 : Training loss: 0.553157031536, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 351 : Training loss: 0.607506930828, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 361 : Training loss: 0.602309048176, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 371 : Training loss: 0.550845980644, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 381 : Training loss: 0.53530728817, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 391 : Training loss: 0.530287563801, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 401 : Training loss: 0.509501218796, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 411 : Training loss: 0.655608475208, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 421 : Training loss: 0.52397531271, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 431 : Training loss: 0.580105960369, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 441 : Training loss: 0.570473134518, \n",
      " test accuracy : 0.714285731316\n",
      "\n",
      "Epoch 451 : Training loss: 0.589916169643, \n",
      " test accuracy : 0.714285731316\n",
      "\n",
      "Epoch 461 : Training loss: 0.536696195602, \n",
      " test accuracy : 0.714285731316\n",
      "\n",
      "Epoch 471 : Training loss: 0.512100994587, \n",
      " test accuracy : 0.728571414948\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3c300d3e6dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/stanleygan/Documents/NN_pima/model.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_test, y_test, lr, num_epochs, batch_size, seed, printResults, returnResults)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0;31m#Set up feed_dict for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0mfeed_dict_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdo\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_list_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                     \u001b[0mfeed_dict_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mfeed_dict_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stanleygan/Documents/NN_pima/model.pyc\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((ph, do))\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0;31m#Set up feed_dict for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0mfeed_dict_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdo\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_list_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                     \u001b[0mfeed_dict_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mfeed_dict_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                                    self._dtype.name)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;31m# Necessary to support Python's collection membership operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "mlp.train(X=X_train_scaled, y=y_train, X_test=X_test_scaled, y_test=y_test, num_epochs=5000, lr=0.0001, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation to choose parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from 'model.pyc'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helper\n",
    "import model\n",
    "reload(helper)\n",
    "reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set 1:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.728571428571\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.735714285714\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.742857142857\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.778571428571\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.739130434783\n",
      "\n",
      "Parameter set 2:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.714285714286\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.721428571429\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.75\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.785714285714\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.688405797101\n",
      "\n",
      "Parameter set 3:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.7\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.7\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.742857142857\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.785714285714\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.724637681159\n",
      "\n",
      "Parameter set 4:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.735714285714\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.728571428571\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.75\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.821428571429\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.746376811594\n",
      "\n",
      "Parameter set 5:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.742857142857\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.75\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.742857142857\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.785714285714\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.782608695652\n",
      "\n",
      "The best cross validation accuracy is: 0.760807453416\n",
      "The parameters which achieve this accuracy are: \n",
      "0: {'activations': ['elu', 'elu'], 'dropout': [0.7, 0.7], 'neurons': [30, 20], 'learning_rate': 0.0003}\n"
     ]
    }
   ],
   "source": [
    "neurons=[[30,20,10],\n",
    "         [50,40,30,20,10],\n",
    "         [30,25,20,15],\n",
    "        [50,30,40],\n",
    "        [30,20],\n",
    "        [40,35,30,25,20,15,10],\n",
    "        [20,10,5,3]]\n",
    "\n",
    "activations=[['relu']*3,\n",
    "             ['relu6']*5,\n",
    "             ['relu6']*4,\n",
    "             ['sigmoid']*3,\n",
    "             ['elu']*2,\n",
    "             ['relu6']*7,\n",
    "             ['tanh']*4\n",
    "            ]\n",
    "\n",
    "dropout = [[0.8]*3,\n",
    "           [0.8]*5,\n",
    "           [0.8]*4,\n",
    "           [0.5]*3,\n",
    "           [0.7]*2,\n",
    "           [0.6]*7,\n",
    "           [0.9]*4\n",
    "          ]\n",
    "\n",
    "lr = [0.0001, 0.001, 0.0005, 0.0001, 0.0003]\n",
    "\n",
    "helper.CV_pipeline(X_train_scaled, y_train, neurons, activations, dropout,\n",
    "                   lr, inputDim=X_train_scaled.shape[1], outputDim=2, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Full Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training loss: 0.686136960983, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 11 : Training loss: 0.64018446207, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 21 : Training loss: 0.579932808876, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 31 : Training loss: 0.553266584873, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 41 : Training loss: 0.543897867203, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 51 : Training loss: 0.505584418774, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 61 : Training loss: 0.494587957859, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 71 : Training loss: 0.462892562151, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 81 : Training loss: 0.465350687504, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 91 : Training loss: 0.450852781534, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 101 : Training loss: 0.514164865017, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 111 : Training loss: 0.473172396421, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 121 : Training loss: 0.505977571011, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 131 : Training loss: 0.497074514627, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 141 : Training loss: 0.463934719563, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 151 : Training loss: 0.498258531094, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 161 : Training loss: 0.467004090548, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 171 : Training loss: 0.472266584635, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 181 : Training loss: 0.44090500474, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 191 : Training loss: 0.461484998465, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 201 : Training loss: 0.473384857178, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 211 : Training loss: 0.456928819418, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 221 : Training loss: 0.494509398937, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 231 : Training loss: 0.474372506142, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 241 : Training loss: 0.442925184965, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 251 : Training loss: 0.512032687664, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 261 : Training loss: 0.472535192966, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 271 : Training loss: 0.457847297192, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 281 : Training loss: 0.45627117157, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 291 : Training loss: 0.474963843822, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 301 : Training loss: 0.480336815119, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 311 : Training loss: 0.470526814461, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 321 : Training loss: 0.458509653807, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 331 : Training loss: 0.457621783018, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 341 : Training loss: 0.440902739763, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 351 : Training loss: 0.481876373291, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 361 : Training loss: 0.46635299921, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 371 : Training loss: 0.460137963295, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 381 : Training loss: 0.475341796875, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 391 : Training loss: 0.489293605089, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 401 : Training loss: 0.472129136324, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 411 : Training loss: 0.455359250307, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 421 : Training loss: 0.447596609592, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 431 : Training loss: 0.469040989876, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 441 : Training loss: 0.433401435614, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 451 : Training loss: 0.454374581575, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 461 : Training loss: 0.42917650938, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 471 : Training loss: 0.486586481333, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 481 : Training loss: 0.470773428679, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 491 : Training loss: 0.455846339464, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 501 : Training loss: 0.47122824192, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 511 : Training loss: 0.460105955601, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 521 : Training loss: 0.47802105546, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 531 : Training loss: 0.456355690956, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 541 : Training loss: 0.474794775248, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 551 : Training loss: 0.467771947384, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 561 : Training loss: 0.428682982922, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 571 : Training loss: 0.455522209406, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 581 : Training loss: 0.430750459433, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 591 : Training loss: 0.447086066008, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 601 : Training loss: 0.4728307724, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 611 : Training loss: 0.42236739397, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 621 : Training loss: 0.428940206766, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 631 : Training loss: 0.47087046504, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 641 : Training loss: 0.44933950901, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 651 : Training loss: 0.462568193674, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 661 : Training loss: 0.479428857565, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 671 : Training loss: 0.425646096468, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 681 : Training loss: 0.435535430908, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 691 : Training loss: 0.440551519394, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 701 : Training loss: 0.449509710073, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 711 : Training loss: 0.462991893291, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 721 : Training loss: 0.445847451687, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 731 : Training loss: 0.446979343891, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 741 : Training loss: 0.447679191828, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 751 : Training loss: 0.450563132763, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 761 : Training loss: 0.473597407341, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 771 : Training loss: 0.46043163538, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 781 : Training loss: 0.462082356215, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 791 : Training loss: 0.474265933037, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 801 : Training loss: 0.429837018251, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 811 : Training loss: 0.433061152697, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 821 : Training loss: 0.44501632452, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 831 : Training loss: 0.445858955383, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 841 : Training loss: 0.444246679544, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 851 : Training loss: 0.432208687067, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 861 : Training loss: 0.41390055418, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 871 : Training loss: 0.427860051394, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 881 : Training loss: 0.467094987631, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 891 : Training loss: 0.436241447926, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 901 : Training loss: 0.413444072008, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 911 : Training loss: 0.421804010868, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 921 : Training loss: 0.428504675627, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 931 : Training loss: 0.404126554728, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 941 : Training loss: 0.452307581902, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 951 : Training loss: 0.450777351856, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 961 : Training loss: 0.453745156527, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 971 : Training loss: 0.445942997932, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 981 : Training loss: 0.43189817667, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 991 : Training loss: 0.435100346804, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1001 : Training loss: 0.489670813084, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1011 : Training loss: 0.419587761164, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1021 : Training loss: 0.417148649693, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1031 : Training loss: 0.43355345726, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1041 : Training loss: 0.420631945133, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1051 : Training loss: 0.457038104534, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1061 : Training loss: 0.410962939262, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1071 : Training loss: 0.462512940168, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1081 : Training loss: 0.417755484581, \n",
      " test accuracy : 0.771428585052\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1091 : Training loss: 0.487567931414, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1101 : Training loss: 0.425065308809, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1111 : Training loss: 0.446215599775, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1121 : Training loss: 0.478051841259, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1131 : Training loss: 0.427397191525, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1141 : Training loss: 0.499608248472, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1151 : Training loss: 0.435139834881, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1161 : Training loss: 0.441682726145, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1171 : Training loss: 0.437714040279, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1181 : Training loss: 0.427188307047, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1191 : Training loss: 0.444981396198, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1201 : Training loss: 0.417027384043, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1211 : Training loss: 0.395704448223, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1221 : Training loss: 0.461577504873, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1231 : Training loss: 0.482752501965, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1241 : Training loss: 0.477888971567, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 1251 : Training loss: 0.403003424406, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1261 : Training loss: 0.429836452007, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1271 : Training loss: 0.439235776663, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1281 : Training loss: 0.461661279202, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1291 : Training loss: 0.420349389315, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1301 : Training loss: 0.43009698391, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1311 : Training loss: 0.440506547689, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1321 : Training loss: 0.405030369759, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1331 : Training loss: 0.422909617424, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1341 : Training loss: 0.444933116436, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1351 : Training loss: 0.401098132133, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1361 : Training loss: 0.419128924608, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1371 : Training loss: 0.431629210711, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1381 : Training loss: 0.3761921525, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1391 : Training loss: 0.400764018297, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1401 : Training loss: 0.433018654585, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1411 : Training loss: 0.476591229439, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1421 : Training loss: 0.399205446243, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1431 : Training loss: 0.411768645048, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1441 : Training loss: 0.432506203651, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1451 : Training loss: 0.399853587151, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1461 : Training loss: 0.408900886774, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1471 : Training loss: 0.439481288195, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1481 : Training loss: 0.394987106323, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1491 : Training loss: 0.419919788837, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1501 : Training loss: 0.431353181601, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1511 : Training loss: 0.428874462843, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1521 : Training loss: 0.407644450665, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1531 : Training loss: 0.451815545559, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1541 : Training loss: 0.407769978046, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1551 : Training loss: 0.427032142878, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1561 : Training loss: 0.424705922604, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1571 : Training loss: 0.405907869339, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1581 : Training loss: 0.407554060221, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1591 : Training loss: 0.42854231596, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1601 : Training loss: 0.434142738581, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1611 : Training loss: 0.437338888645, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1621 : Training loss: 0.457933962345, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1631 : Training loss: 0.445096254349, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1641 : Training loss: 0.408724039793, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1651 : Training loss: 0.428457856178, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1661 : Training loss: 0.385289907455, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1671 : Training loss: 0.389091521502, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1681 : Training loss: 0.393623381853, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1691 : Training loss: 0.419242173433, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1701 : Training loss: 0.388273268938, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1711 : Training loss: 0.410441815853, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1721 : Training loss: 0.439726948738, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1731 : Training loss: 0.441371530294, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1741 : Training loss: 0.369058907032, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1751 : Training loss: 0.421153336763, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1761 : Training loss: 0.477571159601, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1771 : Training loss: 0.421866446733, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1781 : Training loss: 0.418329745531, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1791 : Training loss: 0.412874996662, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1801 : Training loss: 0.448356509209, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1811 : Training loss: 0.405971854925, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1821 : Training loss: 0.426887184381, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1831 : Training loss: 0.434354096651, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1841 : Training loss: 0.411326110363, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1851 : Training loss: 0.449654757977, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1861 : Training loss: 0.424278914928, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1871 : Training loss: 0.402463823557, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1881 : Training loss: 0.41260433197, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1891 : Training loss: 0.429873794317, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1901 : Training loss: 0.397510558367, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1911 : Training loss: 0.430662453175, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 1921 : Training loss: 0.387243211269, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 1931 : Training loss: 0.431468963623, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1941 : Training loss: 0.43148162961, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1951 : Training loss: 0.434598863125, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1961 : Training loss: 0.453040361404, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1971 : Training loss: 0.381801098585, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1981 : Training loss: 0.40293648839, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1991 : Training loss: 0.441611200571, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2001 : Training loss: 0.412206113338, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2011 : Training loss: 0.432421654463, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2021 : Training loss: 0.425041109324, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2031 : Training loss: 0.417815476656, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2041 : Training loss: 0.424590110779, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2051 : Training loss: 0.467471957207, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2061 : Training loss: 0.438351899385, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2071 : Training loss: 0.440192490816, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2081 : Training loss: 0.401448190212, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2091 : Training loss: 0.420941412449, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2101 : Training loss: 0.411928534508, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2111 : Training loss: 0.428961336613, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2121 : Training loss: 0.459219038486, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2131 : Training loss: 0.397883832455, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2141 : Training loss: 0.438692301512, \n",
      " test accuracy : 0.800000011921\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2151 : Training loss: 0.401334971189, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2161 : Training loss: 0.398330718279, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2171 : Training loss: 0.40013885498, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2181 : Training loss: 0.386755257845, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2191 : Training loss: 0.39022603631, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2201 : Training loss: 0.384393334389, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2211 : Training loss: 0.409219175577, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2221 : Training loss: 0.395634442568, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2231 : Training loss: 0.430540204048, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2241 : Training loss: 0.423828393221, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2251 : Training loss: 0.414205104113, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2261 : Training loss: 0.395410329103, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2271 : Training loss: 0.42405205965, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2281 : Training loss: 0.419761061668, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2291 : Training loss: 0.406213134527, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2301 : Training loss: 0.380069464445, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2311 : Training loss: 0.413774430752, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2321 : Training loss: 0.42956712842, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2331 : Training loss: 0.441509753466, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2341 : Training loss: 0.435079187155, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2351 : Training loss: 0.417307287455, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2361 : Training loss: 0.44728538394, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2371 : Training loss: 0.411554396152, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2381 : Training loss: 0.433866739273, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2391 : Training loss: 0.415674209595, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2401 : Training loss: 0.391860991716, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2411 : Training loss: 0.396336585283, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2421 : Training loss: 0.432296514511, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2431 : Training loss: 0.413833111525, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2441 : Training loss: 0.429644048214, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2451 : Training loss: 0.43622943759, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2461 : Training loss: 0.404463529587, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2471 : Training loss: 0.441150546074, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2481 : Training loss: 0.386707395315, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2491 : Training loss: 0.422235488892, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2501 : Training loss: 0.389190405607, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2511 : Training loss: 0.404607504606, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2521 : Training loss: 0.417104810476, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2531 : Training loss: 0.39493855834, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2541 : Training loss: 0.44214501977, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2551 : Training loss: 0.413118869066, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2561 : Training loss: 0.416752070189, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2571 : Training loss: 0.432002067566, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2581 : Training loss: 0.407029747963, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2591 : Training loss: 0.40696015954, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2601 : Training loss: 0.415271192789, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2611 : Training loss: 0.437684357166, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2621 : Training loss: 0.411631494761, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2631 : Training loss: 0.398790121078, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2641 : Training loss: 0.359143435955, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2651 : Training loss: 0.398273348808, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2661 : Training loss: 0.42099827528, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2671 : Training loss: 0.419443190098, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2681 : Training loss: 0.39033216238, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2691 : Training loss: 0.442132025957, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2701 : Training loss: 0.390446096659, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2711 : Training loss: 0.4139341712, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2721 : Training loss: 0.418537914753, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2731 : Training loss: 0.412752211094, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2741 : Training loss: 0.42434361577, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2751 : Training loss: 0.443646997213, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2761 : Training loss: 0.405310034752, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2771 : Training loss: 0.391144424677, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2781 : Training loss: 0.431440889835, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2791 : Training loss: 0.417343437672, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2801 : Training loss: 0.406210273504, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2811 : Training loss: 0.427894890308, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2821 : Training loss: 0.396720826626, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2831 : Training loss: 0.40143764019, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2841 : Training loss: 0.401105672121, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2851 : Training loss: 0.395498156548, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2861 : Training loss: 0.411321729422, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2871 : Training loss: 0.432949006557, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2881 : Training loss: 0.399126976728, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2891 : Training loss: 0.386446177959, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2901 : Training loss: 0.4269567132, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2911 : Training loss: 0.393016606569, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2921 : Training loss: 0.417064607143, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2931 : Training loss: 0.414765745401, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2941 : Training loss: 0.405811756849, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2951 : Training loss: 0.449024319649, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2961 : Training loss: 0.413771003485, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 2971 : Training loss: 0.432543516159, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2981 : Training loss: 0.391536384821, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2991 : Training loss: 0.420138418674, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3001 : Training loss: 0.447586268187, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3011 : Training loss: 0.408893793821, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3021 : Training loss: 0.391020983458, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3031 : Training loss: 0.417315930128, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3041 : Training loss: 0.373876601458, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3051 : Training loss: 0.392300277948, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3061 : Training loss: 0.360879063606, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3071 : Training loss: 0.461607933044, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3081 : Training loss: 0.385454058647, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3091 : Training loss: 0.378479510546, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3101 : Training loss: 0.363055676222, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3111 : Training loss: 0.401813000441, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3121 : Training loss: 0.390352547169, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3131 : Training loss: 0.370339184999, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3141 : Training loss: 0.403037637472, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3151 : Training loss: 0.434985995293, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3161 : Training loss: 0.401364982128, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3171 : Training loss: 0.380801796913, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3181 : Training loss: 0.407508015633, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3191 : Training loss: 0.382351309061, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3201 : Training loss: 0.423579722643, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3211 : Training loss: 0.365106344223, \n",
      " test accuracy : 0.814285695553\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3221 : Training loss: 0.365148663521, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3231 : Training loss: 0.391986966133, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3241 : Training loss: 0.365407824516, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3251 : Training loss: 0.367229104042, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3261 : Training loss: 0.389449983835, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3271 : Training loss: 0.364249587059, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3281 : Training loss: 0.407772451639, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3291 : Training loss: 0.379577040672, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3301 : Training loss: 0.365571975708, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3311 : Training loss: 0.393820554018, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3321 : Training loss: 0.387823432684, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3331 : Training loss: 0.407951712608, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3341 : Training loss: 0.427792102098, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3351 : Training loss: 0.409616261721, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3361 : Training loss: 0.359964877367, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3371 : Training loss: 0.440361350775, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3381 : Training loss: 0.423570424318, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3391 : Training loss: 0.399675935507, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3401 : Training loss: 0.380236059427, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3411 : Training loss: 0.356160610914, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3421 : Training loss: 0.398448824883, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3431 : Training loss: 0.382810801268, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3441 : Training loss: 0.369621157646, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3451 : Training loss: 0.348782837391, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3461 : Training loss: 0.371229112148, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3471 : Training loss: 0.407152235508, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3481 : Training loss: 0.406336247921, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3491 : Training loss: 0.400010675192, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3501 : Training loss: 0.42211163044, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3511 : Training loss: 0.435976982117, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3521 : Training loss: 0.373738408089, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3531 : Training loss: 0.331703901291, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3541 : Training loss: 0.385340154171, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3551 : Training loss: 0.368234097958, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3561 : Training loss: 0.410071998835, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3571 : Training loss: 0.401821583509, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3581 : Training loss: 0.388703614473, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3591 : Training loss: 0.394638836384, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3601 : Training loss: 0.390967994928, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3611 : Training loss: 0.391154021025, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3621 : Training loss: 0.356268912554, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3631 : Training loss: 0.390713602304, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3641 : Training loss: 0.365237951279, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3651 : Training loss: 0.390651315451, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3661 : Training loss: 0.367083460093, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3671 : Training loss: 0.374968528748, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3681 : Training loss: 0.380186915398, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3691 : Training loss: 0.401115179062, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3701 : Training loss: 0.388818502426, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3711 : Training loss: 0.374801903963, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3721 : Training loss: 0.369257569313, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3731 : Training loss: 0.380778580904, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3741 : Training loss: 0.379065006971, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3751 : Training loss: 0.368356049061, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3761 : Training loss: 0.371517747641, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3771 : Training loss: 0.377075076103, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3781 : Training loss: 0.41542339325, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3791 : Training loss: 0.382235139608, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3801 : Training loss: 0.418197542429, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3811 : Training loss: 0.403801620007, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3821 : Training loss: 0.396386235952, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3831 : Training loss: 0.390734732151, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3841 : Training loss: 0.375801980495, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3851 : Training loss: 0.406071037054, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3861 : Training loss: 0.376906573772, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3871 : Training loss: 0.409328490496, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3881 : Training loss: 0.357510864735, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3891 : Training loss: 0.383387863636, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3901 : Training loss: 0.368864119053, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3911 : Training loss: 0.434543937445, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3921 : Training loss: 0.413721472025, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3931 : Training loss: 0.354511201382, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3941 : Training loss: 0.389912247658, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3951 : Training loss: 0.35141775012, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3961 : Training loss: 0.362364470959, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3971 : Training loss: 0.379350394011, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3981 : Training loss: 0.418015658855, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3991 : Training loss: 0.398023158312, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4001 : Training loss: 0.38642603159, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4011 : Training loss: 0.355000585318, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4021 : Training loss: 0.361135900021, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4031 : Training loss: 0.369390249252, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4041 : Training loss: 0.397793710232, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4051 : Training loss: 0.37821611762, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4061 : Training loss: 0.391731262207, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4071 : Training loss: 0.39238730073, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4081 : Training loss: 0.342904508114, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4091 : Training loss: 0.351871043444, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4101 : Training loss: 0.312247604132, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4111 : Training loss: 0.422789543867, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4121 : Training loss: 0.361822396517, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4131 : Training loss: 0.42057672143, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4141 : Training loss: 0.381203085184, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4151 : Training loss: 0.368313372135, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4161 : Training loss: 0.36639302969, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4171 : Training loss: 0.352787107229, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4181 : Training loss: 0.379751950502, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4191 : Training loss: 0.388792067766, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4201 : Training loss: 0.423518776894, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4211 : Training loss: 0.377618700266, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4221 : Training loss: 0.433402687311, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4231 : Training loss: 0.436733692884, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4241 : Training loss: 0.394048273563, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4251 : Training loss: 0.423345834017, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4261 : Training loss: 0.353144586086, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4271 : Training loss: 0.347817242146, \n",
      " test accuracy : 0.800000011921\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4281 : Training loss: 0.432290762663, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4291 : Training loss: 0.384389996529, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4301 : Training loss: 0.376340955496, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4311 : Training loss: 0.379217416048, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4321 : Training loss: 0.316934347153, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4331 : Training loss: 0.3817730546, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4341 : Training loss: 0.361242204905, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4351 : Training loss: 0.400108844042, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4361 : Training loss: 0.370129227638, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4371 : Training loss: 0.378615498543, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4381 : Training loss: 0.408598154783, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4391 : Training loss: 0.36379057169, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4401 : Training loss: 0.386576980352, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4411 : Training loss: 0.32713457942, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4421 : Training loss: 0.418548285961, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4431 : Training loss: 0.354368805885, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4441 : Training loss: 0.399659335613, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4451 : Training loss: 0.36406826973, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4461 : Training loss: 0.378414422274, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4471 : Training loss: 0.364646911621, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4481 : Training loss: 0.445341050625, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4491 : Training loss: 0.357477545738, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4501 : Training loss: 0.405799090862, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4511 : Training loss: 0.390834629536, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4521 : Training loss: 0.357715845108, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4531 : Training loss: 0.390279769897, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4541 : Training loss: 0.35181748867, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4551 : Training loss: 0.4032587111, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4561 : Training loss: 0.369164139032, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4571 : Training loss: 0.38219204545, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4581 : Training loss: 0.382757902145, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4591 : Training loss: 0.368860810995, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4601 : Training loss: 0.377800822258, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4611 : Training loss: 0.335936903954, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4621 : Training loss: 0.321819275618, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4631 : Training loss: 0.377499192953, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4641 : Training loss: 0.377698898315, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4651 : Training loss: 0.371676981449, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4661 : Training loss: 0.315390497446, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4671 : Training loss: 0.395709991455, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4681 : Training loss: 0.397564649582, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4691 : Training loss: 0.360103607178, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4701 : Training loss: 0.407591193914, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4711 : Training loss: 0.348673701286, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4721 : Training loss: 0.332968503237, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4731 : Training loss: 0.349891096354, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4741 : Training loss: 0.381788313389, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4751 : Training loss: 0.342438936234, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4761 : Training loss: 0.358122587204, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4771 : Training loss: 0.330969750881, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4781 : Training loss: 0.337720453739, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4791 : Training loss: 0.342810928822, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4801 : Training loss: 0.367897540331, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4811 : Training loss: 0.395876556635, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4821 : Training loss: 0.409613519907, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4831 : Training loss: 0.351431339979, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4841 : Training loss: 0.386613100767, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4851 : Training loss: 0.374565690756, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4861 : Training loss: 0.403900593519, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4871 : Training loss: 0.376074701548, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4881 : Training loss: 0.346837669611, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4891 : Training loss: 0.402827054262, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4901 : Training loss: 0.373467236757, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4911 : Training loss: 0.326024919748, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4921 : Training loss: 0.316602140665, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4931 : Training loss: 0.35547670722, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4941 : Training loss: 0.338358491659, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4951 : Training loss: 0.379724472761, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4961 : Training loss: 0.354904294014, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4971 : Training loss: 0.355315953493, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4981 : Training loss: 0.345884859562, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4991 : Training loss: 0.359567940235, \n",
      " test accuracy : 0.800000011921\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3a181a950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputDim = X_train_scaled.shape[1]\n",
    "outputDim = y_train.shape[1]\n",
    "mlp = MLP()\n",
    "mlp.buildModel(neurons=[30,20], activations=['elu', 'elu'], \n",
    "               dropout=[0.7,0.7], inputDim=inputDim, outputDim=outputDim)\n",
    "\n",
    "%matplotlib inline\n",
    "results = mlp.train(X=X_train_scaled, y=y_train, X_test=X_test_scaled, y_test=y_test, \n",
    "                    num_epochs=5000, lr=0.0003, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on each test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on each scaled test set and obtain average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy on imputed test sets: 0.8\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = list()\n",
    "\n",
    "for ts in test_split_list:\n",
    "    temp_scaled = scale_X.transform(ts[0])\n",
    "    res = mlp.predict(X=temp_scaled)\n",
    "    accuracy = (res[\"y_pred_cls\"][:,0] == ts[1][:,0]).sum() / float(len(res[\"y_pred_cls\"]))\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "print(\"The average accuracy on imputed test sets: {}\".format(np.mean(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.8, 0.8, 0.8, 0.8]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
