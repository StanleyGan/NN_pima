{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Pima Indian dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import data\n",
    "from model import MLP\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "#TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve imputed train and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentPath = os.getcwd()\n",
    "test_list = data.testImputed(currentPath=currentPath)\n",
    "train = data.trainImputed(currentPath=currentPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Outcome into two columns instead of 1: Diabetic and Not_Diabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For train\n",
    "train[\"Not Diabetic\"] = (train[\"Outcome\"] == 0).astype(int)\n",
    "train.rename(columns={\"Outcome\": \"Diabetic\"}, inplace=True)\n",
    "\n",
    "#For test\n",
    "for i in range(len(test_list)):\n",
    "    test_list[i][\"Not Diabetic\"] = (test_list[i][\"Outcome\"] == 0).astype(int)\n",
    "    test_list[i].rename(columns={\"Outcome\": \"Diabetic\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into target and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.as_matrix()[:,0:-2]\n",
    "y_train = train.as_matrix()[:, -2:]\n",
    "test_split_list = list()\n",
    "\n",
    "#First element is X, second element is y\n",
    "for i in range(len(test_list)):\n",
    "    test_split_list.append( [ test_list[i].as_matrix()[:,0:-2],\n",
    "                         test_list[i].as_matrix()[:,-2:] ] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_split_list[0][0]\n",
    "y_test = test_split_list[0][1]\n",
    "scale_X = StandardScaler()\n",
    "scale_X.fit(X_train)\n",
    "X_train_scaled = scale_X.transform(X_train)\n",
    "X_test_scaled = scale_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate and train a Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "reload(model)\n",
    "from model import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = X_train_scaled.shape[1]\n",
    "outputDim = y_train.shape[1]\n",
    "mlp = MLP()\n",
    "mlp.buildModel(neurons=[10,10,10,10], activations=[\"relu\", \"relu\", \"relu\", \"relu\"], \n",
    "               dropout=[0.5,0.5,0.5,0.5], inputDim=inputDim, outputDim=outputDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training loss: 0.694314301014, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 11 : Training loss: 0.692826628685, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 21 : Training loss: 0.690799951553, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 31 : Training loss: 0.690326809883, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 41 : Training loss: 0.688141107559, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 51 : Training loss: 0.686038613319, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 61 : Training loss: 0.684974908829, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 71 : Training loss: 0.684505045414, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 81 : Training loss: 0.683964371681, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 91 : Training loss: 0.68582701683, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 101 : Training loss: 0.685524642467, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 111 : Training loss: 0.673711836338, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 121 : Training loss: 0.676564574242, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 131 : Training loss: 0.672813475132, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 141 : Training loss: 0.675782322884, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 151 : Training loss: 0.676288664341, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 161 : Training loss: 0.674137473106, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 171 : Training loss: 0.665604352951, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 181 : Training loss: 0.668407380581, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 191 : Training loss: 0.658246517181, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 201 : Training loss: 0.657447814941, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 211 : Training loss: 0.645871639252, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 221 : Training loss: 0.651230096817, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 231 : Training loss: 0.652669012547, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 241 : Training loss: 0.64414036274, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 251 : Training loss: 0.609565138817, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 261 : Training loss: 0.643973171711, \n",
      " test accuracy : 0.699999988079\n",
      "\n",
      "Epoch 271 : Training loss: 0.607221066952, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 281 : Training loss: 0.61324930191, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 291 : Training loss: 0.612631320953, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 301 : Training loss: 0.656457304955, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 311 : Training loss: 0.652921557426, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 321 : Training loss: 0.594386041164, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 331 : Training loss: 0.586550414562, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 341 : Training loss: 0.571166336536, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 351 : Training loss: 0.598727047443, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 361 : Training loss: 0.551859557629, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 371 : Training loss: 0.553994417191, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 381 : Training loss: 0.528147399426, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 391 : Training loss: 0.61010825634, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 401 : Training loss: 0.536363482475, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 411 : Training loss: 0.542965650558, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 421 : Training loss: 0.568051457405, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 431 : Training loss: 0.555923700333, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 441 : Training loss: 0.613049805164, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 451 : Training loss: 0.570357501507, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 461 : Training loss: 0.527745127678, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 471 : Training loss: 0.599536895752, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 481 : Training loss: 0.552100241184, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 491 : Training loss: 0.556676805019, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 501 : Training loss: 0.595048487186, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 511 : Training loss: 0.55749809742, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 521 : Training loss: 0.501199126244, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 531 : Training loss: 0.691476464272, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 541 : Training loss: 0.526873528957, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 551 : Training loss: 0.536001622677, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 561 : Training loss: 0.542451560497, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 571 : Training loss: 0.585275888443, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 581 : Training loss: 0.637072145939, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 591 : Training loss: 0.578439474106, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 601 : Training loss: 0.589691400528, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 611 : Training loss: 0.563260555267, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 621 : Training loss: 0.550405859947, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 631 : Training loss: 0.565191984177, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 641 : Training loss: 0.635108232498, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 651 : Training loss: 0.556744039059, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 661 : Training loss: 0.555834650993, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 671 : Training loss: 0.536247551441, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 681 : Training loss: 0.576136171818, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 691 : Training loss: 0.608420491219, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 701 : Training loss: 0.602933466434, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 711 : Training loss: 0.56860101223, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 721 : Training loss: 0.652234494686, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 731 : Training loss: 0.540222704411, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 741 : Training loss: 0.659693181515, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 751 : Training loss: 0.587619185448, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 761 : Training loss: 0.614132702351, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 771 : Training loss: 0.616030275822, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 781 : Training loss: 0.583294510841, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 791 : Training loss: 0.590784907341, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 801 : Training loss: 0.608078718185, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 811 : Training loss: 0.569322228432, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 821 : Training loss: 0.539062380791, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 831 : Training loss: 0.509144723415, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 841 : Training loss: 0.552167117596, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 851 : Training loss: 0.607102274895, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 861 : Training loss: 0.532737016678, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 871 : Training loss: 0.599064290524, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 881 : Training loss: 0.54957306385, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 891 : Training loss: 0.555535078049, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 901 : Training loss: 0.497415155172, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 911 : Training loss: 0.544054925442, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 921 : Training loss: 0.615259587765, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 931 : Training loss: 0.533786177635, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 941 : Training loss: 0.534142673016, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 951 : Training loss: 0.559161603451, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 961 : Training loss: 0.620593965054, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 971 : Training loss: 0.60860145092, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 981 : Training loss: 0.494577139616, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 991 : Training loss: 0.480351775885, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1001 : Training loss: 0.513306856155, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1011 : Training loss: 0.631091535091, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1021 : Training loss: 0.602500081062, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1031 : Training loss: 0.5894292593, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1041 : Training loss: 0.524875998497, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1051 : Training loss: 0.524114072323, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1061 : Training loss: 0.566941738129, \n",
      " test accuracy : 0.800000011921\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1071 : Training loss: 0.547445714474, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1081 : Training loss: 0.59648501873, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1091 : Training loss: 0.582020163536, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1101 : Training loss: 0.549617111683, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1111 : Training loss: 0.510920166969, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1121 : Training loss: 0.516544699669, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1131 : Training loss: 0.525967955589, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1141 : Training loss: 0.524755418301, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1151 : Training loss: 0.561574876308, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1161 : Training loss: 0.620010137558, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1171 : Training loss: 0.526211738586, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1181 : Training loss: 0.45645159483, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1191 : Training loss: 0.630298793316, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1201 : Training loss: 0.511562883854, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1211 : Training loss: 0.580739557743, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1221 : Training loss: 0.509703993797, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1231 : Training loss: 0.510232567787, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1241 : Training loss: 0.534061491489, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1251 : Training loss: 0.576945662498, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1261 : Training loss: 0.59791636467, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1271 : Training loss: 0.579968035221, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1281 : Training loss: 0.518849909306, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1291 : Training loss: 0.569977402687, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1301 : Training loss: 0.525954425335, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1311 : Training loss: 0.52442753315, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1321 : Training loss: 0.646343052387, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1331 : Training loss: 0.482945978642, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1341 : Training loss: 0.483534365892, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1351 : Training loss: 0.583973169327, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1361 : Training loss: 0.607912838459, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1371 : Training loss: 0.577324151993, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1381 : Training loss: 0.545815229416, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1391 : Training loss: 0.57180929184, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1401 : Training loss: 0.549140870571, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1411 : Training loss: 0.520747840405, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1421 : Training loss: 0.56360989809, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1431 : Training loss: 0.528032898903, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1441 : Training loss: 0.525024175644, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1451 : Training loss: 0.558772861958, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1461 : Training loss: 0.593837976456, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1471 : Training loss: 0.534634828568, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1481 : Training loss: 0.518814563751, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1491 : Training loss: 0.561347424984, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1501 : Training loss: 0.616922438145, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1511 : Training loss: 0.539983332157, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1521 : Training loss: 0.541743159294, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1531 : Training loss: 0.508215844631, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1541 : Training loss: 0.558978021145, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1551 : Training loss: 0.549891293049, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1561 : Training loss: 0.501450657845, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1571 : Training loss: 0.513044595718, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1581 : Training loss: 0.498947918415, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1591 : Training loss: 0.541050493717, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1601 : Training loss: 0.518355607986, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1611 : Training loss: 0.538974046707, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1621 : Training loss: 0.600413262844, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1631 : Training loss: 0.487704217434, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1641 : Training loss: 0.541240870953, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1651 : Training loss: 0.558542072773, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1661 : Training loss: 0.531442761421, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1671 : Training loss: 0.530311822891, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1681 : Training loss: 0.518870413303, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1691 : Training loss: 0.541092395782, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1701 : Training loss: 0.535386681557, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1711 : Training loss: 0.466837674379, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1721 : Training loss: 0.522986352444, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1731 : Training loss: 0.539824783802, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1741 : Training loss: 0.484517157078, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1751 : Training loss: 0.545499205589, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1761 : Training loss: 0.485316008329, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1771 : Training loss: 0.500585436821, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1781 : Training loss: 0.526662230492, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1791 : Training loss: 0.525270700455, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1801 : Training loss: 0.538506865501, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1811 : Training loss: 0.534457325935, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1821 : Training loss: 0.596972584724, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1831 : Training loss: 0.553040802479, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1841 : Training loss: 0.489515930414, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1851 : Training loss: 0.540896356106, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1861 : Training loss: 0.483414888382, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1871 : Training loss: 0.553120493889, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1881 : Training loss: 0.493242889643, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1891 : Training loss: 0.488788485527, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1901 : Training loss: 0.511016428471, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1911 : Training loss: 0.532657265663, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1921 : Training loss: 0.486761838198, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1931 : Training loss: 0.511411011219, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1941 : Training loss: 0.491887181997, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1951 : Training loss: 0.499162971973, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1961 : Training loss: 0.51372218132, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1971 : Training loss: 0.528323709965, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1981 : Training loss: 0.586936891079, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1991 : Training loss: 0.500822842121, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2001 : Training loss: 0.613451063633, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2011 : Training loss: 0.593255639076, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2021 : Training loss: 0.507991552353, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2031 : Training loss: 0.512626290321, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2041 : Training loss: 0.597954511642, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2051 : Training loss: 0.557357430458, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2061 : Training loss: 0.487892925739, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2071 : Training loss: 0.538267970085, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2081 : Training loss: 0.568479478359, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2091 : Training loss: 0.483134359121, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2101 : Training loss: 0.476273566484, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2111 : Training loss: 0.501059710979, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2121 : Training loss: 0.546859264374, \n",
      " test accuracy : 0.800000011921\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2131 : Training loss: 0.479418754578, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2141 : Training loss: 0.490226566792, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2151 : Training loss: 0.553202211857, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2161 : Training loss: 0.534338474274, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2171 : Training loss: 0.556118071079, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2181 : Training loss: 0.518820166588, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2191 : Training loss: 0.54202234745, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2201 : Training loss: 0.57056504488, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2211 : Training loss: 0.551253914833, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2221 : Training loss: 0.486887723207, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2231 : Training loss: 0.613197922707, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2241 : Training loss: 0.511562526226, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2251 : Training loss: 0.519060254097, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2261 : Training loss: 0.513020694256, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2271 : Training loss: 0.500053226948, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2281 : Training loss: 0.512282192707, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2291 : Training loss: 0.486293166876, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2301 : Training loss: 0.471740812063, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2311 : Training loss: 0.548994958401, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2321 : Training loss: 0.544410288334, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2331 : Training loss: 0.558257699013, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2341 : Training loss: 0.569494903088, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2351 : Training loss: 0.563430309296, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2361 : Training loss: 0.534989833832, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2371 : Training loss: 0.474861115217, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2381 : Training loss: 0.533349633217, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2391 : Training loss: 0.503343462944, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2401 : Training loss: 0.534158289433, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2411 : Training loss: 0.627894937992, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2421 : Training loss: 0.467224180698, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2431 : Training loss: 0.487006485462, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2441 : Training loss: 0.527599096298, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2451 : Training loss: 0.448320001364, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2461 : Training loss: 0.463597297668, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2471 : Training loss: 0.520078063011, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2481 : Training loss: 0.549567997456, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2491 : Training loss: 0.53332477808, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2501 : Training loss: 0.491920143366, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2511 : Training loss: 0.491987019777, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2521 : Training loss: 0.514786303043, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2531 : Training loss: 0.495963156223, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2541 : Training loss: 0.513360500336, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2551 : Training loss: 0.586873233318, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2561 : Training loss: 0.464697420597, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2571 : Training loss: 0.561442196369, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2581 : Training loss: 0.549999237061, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2591 : Training loss: 0.569459140301, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2601 : Training loss: 0.56150072813, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2611 : Training loss: 0.498648107052, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2621 : Training loss: 0.564420640469, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2631 : Training loss: 0.498439908028, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2641 : Training loss: 0.507729053497, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2651 : Training loss: 0.513269543648, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2661 : Training loss: 0.557951033115, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2671 : Training loss: 0.501209616661, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2681 : Training loss: 0.462781816721, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2691 : Training loss: 0.482154786587, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2701 : Training loss: 0.518236160278, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2711 : Training loss: 0.61812710762, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2721 : Training loss: 0.560872256756, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2731 : Training loss: 0.552762925625, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2741 : Training loss: 0.641269028187, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2751 : Training loss: 0.494563192129, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2761 : Training loss: 0.470521211624, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2771 : Training loss: 0.550535976887, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2781 : Training loss: 0.647025465965, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2791 : Training loss: 0.487107276917, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2801 : Training loss: 0.471472352743, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2811 : Training loss: 0.548277497292, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2821 : Training loss: 0.524381995201, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2831 : Training loss: 0.453469574451, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2841 : Training loss: 0.507989645004, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2851 : Training loss: 0.612679123878, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2861 : Training loss: 0.500623345375, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2871 : Training loss: 0.494020283222, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2881 : Training loss: 0.483639419079, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2891 : Training loss: 0.583243906498, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2901 : Training loss: 0.521694302559, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2911 : Training loss: 0.545728206635, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2921 : Training loss: 0.528098225594, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2931 : Training loss: 0.526440739632, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2941 : Training loss: 0.526368737221, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2951 : Training loss: 0.514544963837, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2961 : Training loss: 0.490331292152, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2971 : Training loss: 0.46471169591, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2981 : Training loss: 0.476843446493, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2991 : Training loss: 0.540569543839, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3001 : Training loss: 0.464202284813, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3011 : Training loss: 0.539952337742, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3021 : Training loss: 0.551068544388, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3031 : Training loss: 0.491492688656, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3041 : Training loss: 0.507791638374, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3051 : Training loss: 0.480571061373, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3061 : Training loss: 0.512126684189, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3071 : Training loss: 0.543590068817, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3081 : Training loss: 0.520218849182, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3091 : Training loss: 0.549637794495, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3101 : Training loss: 0.545625209808, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3111 : Training loss: 0.487456679344, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3121 : Training loss: 0.465978384018, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3131 : Training loss: 0.488631665707, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3141 : Training loss: 0.508876621723, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3151 : Training loss: 0.453515976667, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3161 : Training loss: 0.562792897224, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3171 : Training loss: 0.467778593302, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3181 : Training loss: 0.465219169855, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3191 : Training loss: 0.480637192726, \n",
      " test accuracy : 0.785714268684\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3201 : Training loss: 0.501706898212, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3211 : Training loss: 0.550597906113, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3221 : Training loss: 0.470423609018, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3231 : Training loss: 0.507527410984, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3241 : Training loss: 0.550493299961, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3251 : Training loss: 0.455105006695, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3261 : Training loss: 0.43584176898, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3271 : Training loss: 0.452228456736, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3281 : Training loss: 0.469605654478, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3291 : Training loss: 0.525693833828, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3301 : Training loss: 0.547101974487, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3311 : Training loss: 0.485591113567, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3321 : Training loss: 0.513615727425, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3331 : Training loss: 0.510803580284, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3341 : Training loss: 0.489429175854, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3351 : Training loss: 0.414843678474, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3361 : Training loss: 0.514937400818, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3371 : Training loss: 0.52863496542, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3381 : Training loss: 0.626749813557, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3391 : Training loss: 0.513843953609, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3401 : Training loss: 0.502309560776, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3411 : Training loss: 0.490263193846, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3421 : Training loss: 0.486050516367, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3431 : Training loss: 0.519491970539, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3441 : Training loss: 0.460798144341, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3451 : Training loss: 0.422482699156, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3461 : Training loss: 0.450838506222, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3471 : Training loss: 0.542760848999, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3481 : Training loss: 0.503455460072, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3491 : Training loss: 0.514718234539, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3501 : Training loss: 0.49866065383, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3511 : Training loss: 0.480353683233, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3521 : Training loss: 0.517561972141, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3531 : Training loss: 0.441606074572, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3541 : Training loss: 0.437529295683, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3551 : Training loss: 0.493534296751, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3561 : Training loss: 0.543348968029, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3571 : Training loss: 0.489780694246, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3581 : Training loss: 0.472534537315, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3591 : Training loss: 0.428620785475, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3601 : Training loss: 0.518082559109, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3611 : Training loss: 0.534249663353, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3621 : Training loss: 0.515518724918, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3631 : Training loss: 0.477433890104, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3641 : Training loss: 0.572443127632, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3651 : Training loss: 0.525084912777, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3661 : Training loss: 0.539072334766, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3671 : Training loss: 0.490891009569, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3681 : Training loss: 0.520593643188, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3691 : Training loss: 0.473252356052, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3701 : Training loss: 0.50558757782, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3711 : Training loss: 0.492903888226, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3721 : Training loss: 0.541552126408, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3731 : Training loss: 0.455508172512, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3741 : Training loss: 0.482346475124, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3751 : Training loss: 0.471748888493, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3761 : Training loss: 0.489956855774, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3771 : Training loss: 0.529640316963, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3781 : Training loss: 0.493048042059, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3791 : Training loss: 0.473799139261, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3801 : Training loss: 0.487785041332, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 3811 : Training loss: 0.493289381266, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3821 : Training loss: 0.45011049509, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3831 : Training loss: 0.507819473743, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3841 : Training loss: 0.50502127409, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3851 : Training loss: 0.543470919132, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3861 : Training loss: 0.442220270634, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3871 : Training loss: 0.516988813877, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3881 : Training loss: 0.480584979057, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3891 : Training loss: 0.50868922472, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3901 : Training loss: 0.505199730396, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3911 : Training loss: 0.51955974102, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3921 : Training loss: 0.516143918037, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3931 : Training loss: 0.598836243153, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3941 : Training loss: 0.508175313473, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3951 : Training loss: 0.51357537508, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3961 : Training loss: 0.492255628109, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3971 : Training loss: 0.56157541275, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3981 : Training loss: 0.465641766787, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 3991 : Training loss: 0.498169392347, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4001 : Training loss: 0.435515165329, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4011 : Training loss: 0.502531707287, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4021 : Training loss: 0.472714811563, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4031 : Training loss: 0.483755022287, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4041 : Training loss: 0.631239712238, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4051 : Training loss: 0.497050464153, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4061 : Training loss: 0.476292222738, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4071 : Training loss: 0.487180054188, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4081 : Training loss: 0.656692445278, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4091 : Training loss: 0.595680952072, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4101 : Training loss: 0.468391746283, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4111 : Training loss: 0.480476945639, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4121 : Training loss: 0.538692772388, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4131 : Training loss: 0.497751444578, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4141 : Training loss: 0.478751778603, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4151 : Training loss: 0.525887608528, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4161 : Training loss: 0.492108732462, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4171 : Training loss: 0.517717897892, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4181 : Training loss: 0.594891607761, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4191 : Training loss: 0.519365787506, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4201 : Training loss: 0.512014269829, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4211 : Training loss: 0.455915719271, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4221 : Training loss: 0.495756477118, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4231 : Training loss: 0.490126401186, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4241 : Training loss: 0.476604282856, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4251 : Training loss: 0.488350600004, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4261 : Training loss: 0.503499448299, \n",
      " test accuracy : 0.785714268684\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4271 : Training loss: 0.554853856564, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4281 : Training loss: 0.454265773296, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4291 : Training loss: 0.518540024757, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4301 : Training loss: 0.498376190662, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4311 : Training loss: 0.492559671402, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4321 : Training loss: 0.461686879396, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4331 : Training loss: 0.515039682388, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4341 : Training loss: 0.513954818249, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4351 : Training loss: 0.516654789448, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4361 : Training loss: 0.557469069958, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4371 : Training loss: 0.617254078388, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4381 : Training loss: 0.470565140247, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4391 : Training loss: 0.558257699013, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4401 : Training loss: 0.426323503256, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4411 : Training loss: 0.4585981071, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4421 : Training loss: 0.447700768709, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4431 : Training loss: 0.458761334419, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4441 : Training loss: 0.477280169725, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4451 : Training loss: 0.45288208127, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4461 : Training loss: 0.599538862705, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4471 : Training loss: 0.469908922911, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4481 : Training loss: 0.55692255497, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4491 : Training loss: 0.434498816729, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4501 : Training loss: 0.523053884506, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4511 : Training loss: 0.572034955025, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4521 : Training loss: 0.533380866051, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4531 : Training loss: 0.500326991081, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4541 : Training loss: 0.586941242218, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4551 : Training loss: 0.569421768188, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4561 : Training loss: 0.527712583542, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4571 : Training loss: 0.505601763725, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4581 : Training loss: 0.45264044404, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4591 : Training loss: 0.408428251743, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4601 : Training loss: 0.425983965397, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4611 : Training loss: 0.503317654133, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4621 : Training loss: 0.426769316196, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4631 : Training loss: 0.457965135574, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4641 : Training loss: 0.46789842844, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4651 : Training loss: 0.448236703873, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4661 : Training loss: 0.512905418873, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4671 : Training loss: 0.46947774291, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4681 : Training loss: 0.476226240396, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4691 : Training loss: 0.441488206387, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4701 : Training loss: 0.487120240927, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4711 : Training loss: 0.487599879503, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4721 : Training loss: 0.432588160038, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4731 : Training loss: 0.459507703781, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4741 : Training loss: 0.473166912794, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4751 : Training loss: 0.445878207684, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4761 : Training loss: 0.482501477003, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4771 : Training loss: 0.464834928513, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4781 : Training loss: 0.538435578346, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4791 : Training loss: 0.553520858288, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4801 : Training loss: 0.554359078407, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4811 : Training loss: 0.524569094181, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4821 : Training loss: 0.718012750149, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4831 : Training loss: 0.477228194475, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4841 : Training loss: 0.488784015179, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4851 : Training loss: 0.532392919064, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4861 : Training loss: 0.467327803373, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4871 : Training loss: 0.451454132795, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4881 : Training loss: 0.504343152046, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4891 : Training loss: 0.557106614113, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4901 : Training loss: 0.449992060661, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4911 : Training loss: 0.532157897949, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4921 : Training loss: 0.536262333393, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4931 : Training loss: 0.438769340515, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4941 : Training loss: 0.532997488976, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4951 : Training loss: 0.4886739254, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4961 : Training loss: 0.487398117781, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4971 : Training loss: 0.503635942936, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4981 : Training loss: 0.436465352774, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4991 : Training loss: 0.496569812298, \n",
      " test accuracy : 0.785714268684\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3979572a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "mlp.train(X=X_train_scaled, y=y_train, X_test=X_test_scaled, y_test=y_test, num_epochs=5000, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = mlp.predict(X=scale_X.transform(test_split_list[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "print(a[:,0] == test_split_list[0][1][:,0]).sum()/70.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
