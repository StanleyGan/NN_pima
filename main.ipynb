{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Pima Indian dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import data\n",
    "from model import MLP\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "seed=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "#TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve imputed train and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentPath = os.getcwd()\n",
    "test_list = data.testImputed(currentPath=currentPath)\n",
    "train = data.trainImputed(currentPath=currentPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Outcome into two columns instead of 1: Diabetic and Not_Diabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For train\n",
    "train[\"Not Diabetic\"] = (train[\"Outcome\"] == 0).astype(int)\n",
    "train.rename(columns={\"Outcome\": \"Diabetic\"}, inplace=True)\n",
    "\n",
    "#For test\n",
    "for i in range(len(test_list)):\n",
    "    test_list[i][\"Not Diabetic\"] = (test_list[i][\"Outcome\"] == 0).astype(int)\n",
    "    test_list[i].rename(columns={\"Outcome\": \"Diabetic\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into target and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.as_matrix()[:,0:-2]\n",
    "y_train = train.as_matrix()[:, -2:]\n",
    "test_split_list = list()\n",
    "\n",
    "#First element is X, second element is y\n",
    "for i in range(len(test_list)):\n",
    "    test_split_list.append( [ test_list[i].as_matrix()[:,0:-2],\n",
    "                         test_list[i].as_matrix()[:,-2:] ] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_split_list[0][0]\n",
    "y_test = test_split_list[0][1]\n",
    "scale_X = StandardScaler()\n",
    "scale_X.fit(X_train)\n",
    "X_train_scaled = scale_X.transform(X_train)\n",
    "X_test_scaled = scale_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate and train a Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "reload(model)\n",
    "from model import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = X_train_scaled.shape[1]\n",
    "outputDim = y_train.shape[1]\n",
    "mlp = MLP()\n",
    "mlp.buildModel(neurons=[10,10,10,10], activations=[\"relu\", \"relu\", \"relu\", \"relu\"], \n",
    "               dropout=[0.5,0.5,0.5,0.5], inputDim=inputDim, outputDim=outputDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training loss: 0.706515908241, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 11 : Training loss: 0.704764485359, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 21 : Training loss: 0.702696740627, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 31 : Training loss: 0.700844526291, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 41 : Training loss: 0.697329103947, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 51 : Training loss: 0.696234583855, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 61 : Training loss: 0.692880511284, \n",
      " test accuracy : 0.342857152224\n",
      "\n",
      "Epoch 71 : Training loss: 0.689456522465, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 81 : Training loss: 0.690316081047, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 91 : Training loss: 0.683042347431, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 101 : Training loss: 0.687663972378, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 111 : Training loss: 0.680644869804, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 121 : Training loss: 0.677502691746, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 131 : Training loss: 0.666579425335, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 141 : Training loss: 0.673410415649, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 151 : Training loss: 0.669799625874, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 161 : Training loss: 0.662181317806, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 171 : Training loss: 0.669597387314, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 181 : Training loss: 0.663963735104, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 191 : Training loss: 0.649111390114, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 201 : Training loss: 0.642676055431, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 211 : Training loss: 0.604106128216, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 221 : Training loss: 0.618971765041, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 231 : Training loss: 0.599858343601, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 241 : Training loss: 0.60164141655, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 251 : Training loss: 0.610950529575, \n",
      " test accuracy : 0.657142877579\n",
      "\n",
      "Epoch 261 : Training loss: 0.606283009052, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 271 : Training loss: 0.595638692379, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 281 : Training loss: 0.630593121052, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 291 : Training loss: 0.582282960415, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 301 : Training loss: 0.54444038868, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 311 : Training loss: 0.624089837074, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 321 : Training loss: 0.58918941021, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 331 : Training loss: 0.562024235725, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 341 : Training loss: 0.623160421848, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 351 : Training loss: 0.633232355118, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 361 : Training loss: 0.572537302971, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 371 : Training loss: 0.559137105942, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 381 : Training loss: 0.530272126198, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 391 : Training loss: 0.552326858044, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 401 : Training loss: 0.586108326912, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 411 : Training loss: 0.584522724152, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 421 : Training loss: 0.522122323513, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 431 : Training loss: 0.565301239491, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 441 : Training loss: 0.560639321804, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 451 : Training loss: 0.523426830769, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 461 : Training loss: 0.594077289104, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 471 : Training loss: 0.542023003101, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 481 : Training loss: 0.538111269474, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 491 : Training loss: 0.576770186424, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 501 : Training loss: 0.571034789085, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 511 : Training loss: 0.533903717995, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 521 : Training loss: 0.498672753572, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 531 : Training loss: 0.621587574482, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 541 : Training loss: 0.549381732941, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 551 : Training loss: 0.590662658215, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 561 : Training loss: 0.537024199963, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 571 : Training loss: 0.602749288082, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 581 : Training loss: 0.589080810547, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 591 : Training loss: 0.56560498476, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 601 : Training loss: 0.611538589001, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 611 : Training loss: 0.602429628372, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 621 : Training loss: 0.544384598732, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 631 : Training loss: 0.485539168119, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 641 : Training loss: 0.593792974949, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 651 : Training loss: 0.561063945293, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 661 : Training loss: 0.580112814903, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 671 : Training loss: 0.603796601295, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 681 : Training loss: 0.568198263645, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 691 : Training loss: 0.488660573959, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 701 : Training loss: 0.531295418739, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 711 : Training loss: 0.496491789818, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 721 : Training loss: 0.615479290485, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 731 : Training loss: 0.586226403713, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 741 : Training loss: 0.510996639729, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 751 : Training loss: 0.522620558739, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 761 : Training loss: 0.545411288738, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 771 : Training loss: 0.520906686783, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 781 : Training loss: 0.537436246872, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 791 : Training loss: 0.556242406368, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 801 : Training loss: 0.530877232552, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 811 : Training loss: 0.486064225435, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 821 : Training loss: 0.558657884598, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 831 : Training loss: 0.523826062679, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 841 : Training loss: 0.475134432316, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 851 : Training loss: 0.446972668171, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 861 : Training loss: 0.571395874023, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 871 : Training loss: 0.626694440842, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 881 : Training loss: 0.525979578495, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 891 : Training loss: 0.525694787502, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 901 : Training loss: 0.615615785122, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 911 : Training loss: 0.564568161964, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 921 : Training loss: 0.626946508884, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 931 : Training loss: 0.496961355209, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 941 : Training loss: 0.479419827461, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 951 : Training loss: 0.566526472569, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 961 : Training loss: 0.475553333759, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 971 : Training loss: 0.532367289066, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 981 : Training loss: 0.586560368538, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 991 : Training loss: 0.620825231075, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1001 : Training loss: 0.554166197777, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1011 : Training loss: 0.523741304874, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1021 : Training loss: 0.432634741068, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1031 : Training loss: 0.570024907589, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1041 : Training loss: 0.571493804455, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1051 : Training loss: 0.535321354866, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1061 : Training loss: 0.523790240288, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1071 : Training loss: 0.512268304825, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1081 : Training loss: 0.554431676865, \n",
      " test accuracy : 0.742857158184\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1091 : Training loss: 0.55999815464, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1101 : Training loss: 0.580728054047, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1111 : Training loss: 0.58830499649, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1121 : Training loss: 0.421236753464, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1131 : Training loss: 0.529187977314, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1141 : Training loss: 0.557278335094, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1151 : Training loss: 0.50847786665, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1161 : Training loss: 0.523816227913, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1171 : Training loss: 0.5902312994, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1181 : Training loss: 0.556781291962, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1191 : Training loss: 0.48606967926, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1201 : Training loss: 0.606019198895, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1211 : Training loss: 0.497242242098, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1221 : Training loss: 0.601434767246, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1231 : Training loss: 0.519243240356, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1241 : Training loss: 0.523465633392, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1251 : Training loss: 0.525522351265, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1261 : Training loss: 0.47980850935, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1271 : Training loss: 0.49000865221, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1281 : Training loss: 0.546639680862, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1291 : Training loss: 0.564438343048, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1301 : Training loss: 0.483279973269, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1311 : Training loss: 0.531393647194, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1321 : Training loss: 0.516859591007, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1331 : Training loss: 0.532243609428, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1341 : Training loss: 0.545834422112, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1351 : Training loss: 0.53520822525, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1361 : Training loss: 0.591417193413, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1371 : Training loss: 0.475660711527, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1381 : Training loss: 0.544931173325, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1391 : Training loss: 0.508010089397, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1401 : Training loss: 0.473767846823, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1411 : Training loss: 0.553735136986, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1421 : Training loss: 0.61473095417, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1431 : Training loss: 0.522203505039, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1441 : Training loss: 0.558483183384, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1451 : Training loss: 0.589933395386, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1461 : Training loss: 0.54110455513, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1471 : Training loss: 0.569707632065, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1481 : Training loss: 0.553934276104, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1491 : Training loss: 0.602800190449, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1501 : Training loss: 0.525565326214, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1511 : Training loss: 0.528651118279, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1521 : Training loss: 0.569357097149, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1531 : Training loss: 0.473291128874, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1541 : Training loss: 0.569984674454, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1551 : Training loss: 0.546912968159, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1561 : Training loss: 0.513356387615, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1571 : Training loss: 0.520154237747, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1581 : Training loss: 0.51995831728, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1591 : Training loss: 0.554239988327, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1601 : Training loss: 0.515807271004, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1611 : Training loss: 0.501390814781, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1621 : Training loss: 0.560752272606, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1631 : Training loss: 0.552908718586, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1641 : Training loss: 0.518032848835, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1651 : Training loss: 0.476338118315, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1661 : Training loss: 0.506104946136, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1671 : Training loss: 0.578707635403, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1681 : Training loss: 0.479639917612, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1691 : Training loss: 0.537447333336, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1701 : Training loss: 0.598440766335, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1711 : Training loss: 0.552420675755, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1721 : Training loss: 0.507374405861, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1731 : Training loss: 0.541304409504, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1741 : Training loss: 0.504262506962, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1751 : Training loss: 0.560841917992, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1761 : Training loss: 0.579151690006, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1771 : Training loss: 0.485666453838, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1781 : Training loss: 0.498705774546, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1791 : Training loss: 0.57466083765, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1801 : Training loss: 0.45426979661, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1811 : Training loss: 0.546130895615, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1821 : Training loss: 0.502517223358, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1831 : Training loss: 0.468002289534, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1841 : Training loss: 0.531000852585, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1851 : Training loss: 0.51003330946, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1861 : Training loss: 0.542296528816, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1871 : Training loss: 0.535976171494, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1881 : Training loss: 0.532775342464, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1891 : Training loss: 0.552302479744, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1901 : Training loss: 0.490476071835, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1911 : Training loss: 0.494087517262, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1921 : Training loss: 0.526910722256, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1931 : Training loss: 0.47756677866, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1941 : Training loss: 0.578582882881, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1951 : Training loss: 0.521679997444, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1961 : Training loss: 0.564016699791, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 1971 : Training loss: 0.500248730183, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1981 : Training loss: 0.526161432266, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 1991 : Training loss: 0.519133508205, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2001 : Training loss: 0.564579188824, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2011 : Training loss: 0.545312047005, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2021 : Training loss: 0.561658740044, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2031 : Training loss: 0.577349543571, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2041 : Training loss: 0.590047776699, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2051 : Training loss: 0.551764190197, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2061 : Training loss: 0.535576224327, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2071 : Training loss: 0.505968809128, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2081 : Training loss: 0.575042903423, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2091 : Training loss: 0.513619542122, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2101 : Training loss: 0.575933396816, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2111 : Training loss: 0.555475890636, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2121 : Training loss: 0.561696410179, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2131 : Training loss: 0.505744874477, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2141 : Training loss: 0.572377681732, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2151 : Training loss: 0.549990236759, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2161 : Training loss: 0.543768584728, \n",
      " test accuracy : 0.742857158184\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2171 : Training loss: 0.510122478008, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2181 : Training loss: 0.571035861969, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2191 : Training loss: 0.523146867752, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2201 : Training loss: 0.47478556633, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2211 : Training loss: 0.519850850105, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2221 : Training loss: 0.526078999043, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2231 : Training loss: 0.509840905666, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2241 : Training loss: 0.554421722889, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2251 : Training loss: 0.496166974306, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2261 : Training loss: 0.471783548594, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2271 : Training loss: 0.512223303318, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2281 : Training loss: 0.546135008335, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2291 : Training loss: 0.514976620674, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2301 : Training loss: 0.542277693748, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2311 : Training loss: 0.546191990376, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2321 : Training loss: 0.554865598679, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2331 : Training loss: 0.594774603844, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2341 : Training loss: 0.526831448078, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2351 : Training loss: 0.481724023819, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2361 : Training loss: 0.486029446125, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2371 : Training loss: 0.478176832199, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2381 : Training loss: 0.505101144314, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2391 : Training loss: 0.52789080143, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2401 : Training loss: 0.50192463398, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2411 : Training loss: 0.523658692837, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2421 : Training loss: 0.534842550755, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2431 : Training loss: 0.532964646816, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2441 : Training loss: 0.577601075172, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2451 : Training loss: 0.524571299553, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2461 : Training loss: 0.509110093117, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2471 : Training loss: 0.537798464298, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2481 : Training loss: 0.488337635994, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2491 : Training loss: 0.491653710604, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2501 : Training loss: 0.479673504829, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2511 : Training loss: 0.553980648518, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2521 : Training loss: 0.481101989746, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2531 : Training loss: 0.468523293734, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2541 : Training loss: 0.48714992404, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2551 : Training loss: 0.460674315691, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2561 : Training loss: 0.587208867073, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2571 : Training loss: 0.572462260723, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2581 : Training loss: 0.483604758978, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2591 : Training loss: 0.510521829128, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2601 : Training loss: 0.440119087696, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2611 : Training loss: 0.478420883417, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2621 : Training loss: 0.58814483881, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2631 : Training loss: 0.533826351166, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2641 : Training loss: 0.509681165218, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2651 : Training loss: 0.526006996632, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2661 : Training loss: 0.501695513725, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2671 : Training loss: 0.468851596117, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2681 : Training loss: 0.449474602938, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2691 : Training loss: 0.581986367702, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2701 : Training loss: 0.480466336012, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2711 : Training loss: 0.491822808981, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2721 : Training loss: 0.499019593, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2731 : Training loss: 0.472224414349, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2741 : Training loss: 0.544751405716, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2751 : Training loss: 0.489558935165, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2761 : Training loss: 0.559325635433, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2771 : Training loss: 0.555548369884, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2781 : Training loss: 0.591696321964, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2791 : Training loss: 0.507588267326, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 2801 : Training loss: 0.507185161114, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2811 : Training loss: 0.546837449074, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2821 : Training loss: 0.559330821037, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2831 : Training loss: 0.536847174168, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2841 : Training loss: 0.545726895332, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2851 : Training loss: 0.547924458981, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2861 : Training loss: 0.554422557354, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2871 : Training loss: 0.567693769932, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2881 : Training loss: 0.522234737873, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2891 : Training loss: 0.526091516018, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2901 : Training loss: 0.515355825424, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2911 : Training loss: 0.513217031956, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2921 : Training loss: 0.513364732265, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2931 : Training loss: 0.533072113991, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 2941 : Training loss: 0.5710490942, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2951 : Training loss: 0.464990288019, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2961 : Training loss: 0.521542131901, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2971 : Training loss: 0.518527388573, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2981 : Training loss: 0.523101687431, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 2991 : Training loss: 0.5005890131, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 3001 : Training loss: 0.501680374146, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3011 : Training loss: 0.513128519058, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3021 : Training loss: 0.478024572134, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 3031 : Training loss: 0.549851000309, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3041 : Training loss: 0.556954264641, \n",
      " test accuracy : 0.728571414948\n",
      "\n",
      "Epoch 3051 : Training loss: 0.5197006464, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3061 : Training loss: 0.5042963624, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3071 : Training loss: 0.501670598984, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3081 : Training loss: 0.490654498339, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3091 : Training loss: 0.604350805283, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3101 : Training loss: 0.498879134655, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3111 : Training loss: 0.501317739487, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3121 : Training loss: 0.514781534672, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3131 : Training loss: 0.477768331766, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3141 : Training loss: 0.543476939201, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3151 : Training loss: 0.470493376255, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3161 : Training loss: 0.507150709629, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3171 : Training loss: 0.514678537846, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3181 : Training loss: 0.52651488781, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3191 : Training loss: 0.581639170647, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3201 : Training loss: 0.49303534627, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3211 : Training loss: 0.495196342468, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3221 : Training loss: 0.50249683857, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3231 : Training loss: 0.566352248192, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3241 : Training loss: 0.516731441021, \n",
      " test accuracy : 0.742857158184\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3251 : Training loss: 0.514011442661, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3261 : Training loss: 0.584486186504, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3271 : Training loss: 0.548297166824, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3281 : Training loss: 0.575819849968, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3291 : Training loss: 0.456868112087, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3301 : Training loss: 0.494415521622, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3311 : Training loss: 0.525944948196, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3321 : Training loss: 0.535638511181, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3331 : Training loss: 0.494950801134, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3341 : Training loss: 0.587163031101, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3351 : Training loss: 0.527218639851, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3361 : Training loss: 0.48192948103, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3371 : Training loss: 0.535349309444, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3381 : Training loss: 0.46899086237, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3391 : Training loss: 0.494821220636, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3401 : Training loss: 0.460323125124, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3411 : Training loss: 0.513280153275, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3421 : Training loss: 0.495665550232, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3431 : Training loss: 0.483116179705, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3441 : Training loss: 0.518746256828, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3451 : Training loss: 0.561976730824, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3461 : Training loss: 0.488931059837, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3471 : Training loss: 0.510203540325, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3481 : Training loss: 0.584822058678, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3491 : Training loss: 0.487810254097, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3501 : Training loss: 0.523265123367, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3511 : Training loss: 0.648016035557, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3521 : Training loss: 0.492384642363, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3531 : Training loss: 0.575856804848, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3541 : Training loss: 0.497077584267, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3551 : Training loss: 0.458353966475, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3561 : Training loss: 0.499393820763, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 3571 : Training loss: 0.563467264175, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3581 : Training loss: 0.48785763979, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3591 : Training loss: 0.480992376804, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3601 : Training loss: 0.526147961617, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3611 : Training loss: 0.511688888073, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3621 : Training loss: 0.452837496996, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3631 : Training loss: 0.593787968159, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3641 : Training loss: 0.536925196648, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3651 : Training loss: 0.484618008137, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3661 : Training loss: 0.490397840738, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3671 : Training loss: 0.526319921017, \n",
      " test accuracy : 0.757142841816\n",
      "\n",
      "Epoch 3681 : Training loss: 0.547043919563, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3691 : Training loss: 0.573248803616, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3701 : Training loss: 0.47625246644, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3711 : Training loss: 0.499269157648, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3721 : Training loss: 0.511767268181, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3731 : Training loss: 0.49638235569, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3741 : Training loss: 0.444677919149, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3751 : Training loss: 0.52003467083, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3761 : Training loss: 0.540527760983, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3771 : Training loss: 0.502104997635, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3781 : Training loss: 0.462162345648, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3791 : Training loss: 0.539299070835, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3801 : Training loss: 0.482870966196, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3811 : Training loss: 0.528644144535, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3821 : Training loss: 0.548328876495, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3831 : Training loss: 0.483098298311, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3841 : Training loss: 0.505565047264, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3851 : Training loss: 0.433571636677, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3861 : Training loss: 0.546049654484, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3871 : Training loss: 0.512234210968, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3881 : Training loss: 0.460798889399, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3891 : Training loss: 0.42758500576, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3901 : Training loss: 0.49493303895, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3911 : Training loss: 0.454209417105, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3921 : Training loss: 0.503635764122, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3931 : Training loss: 0.461740553379, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3941 : Training loss: 0.559523880482, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3951 : Training loss: 0.488261342049, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3961 : Training loss: 0.498856157064, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3971 : Training loss: 0.414782196283, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3981 : Training loss: 0.504857420921, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 3991 : Training loss: 0.523014068604, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4001 : Training loss: 0.484077870846, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4011 : Training loss: 0.456723481417, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4021 : Training loss: 0.472963511944, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4031 : Training loss: 0.441414624453, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4041 : Training loss: 0.560329854488, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4051 : Training loss: 0.438546776772, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4061 : Training loss: 0.52443498373, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4071 : Training loss: 0.544520318508, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4081 : Training loss: 0.523515582085, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4091 : Training loss: 0.547391295433, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4101 : Training loss: 0.479822158813, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4111 : Training loss: 0.443333208561, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4121 : Training loss: 0.479195296764, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4131 : Training loss: 0.509914457798, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4141 : Training loss: 0.456952512264, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4151 : Training loss: 0.469113260508, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4161 : Training loss: 0.482463896275, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4171 : Training loss: 0.512117266655, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4181 : Training loss: 0.473822832108, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4191 : Training loss: 0.509525895119, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4201 : Training loss: 0.473033279181, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4211 : Training loss: 0.496237665415, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4221 : Training loss: 0.438475906849, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4231 : Training loss: 0.485097885132, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4241 : Training loss: 0.444409936666, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4251 : Training loss: 0.489711463451, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4261 : Training loss: 0.497537195683, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4271 : Training loss: 0.445626944304, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4281 : Training loss: 0.501701235771, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4291 : Training loss: 0.518128216267, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4301 : Training loss: 0.420329123735, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4311 : Training loss: 0.461684197187, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4321 : Training loss: 0.485362440348, \n",
      " test accuracy : 0.771428585052\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4331 : Training loss: 0.531469166279, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4341 : Training loss: 0.474008023739, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4351 : Training loss: 0.498501867056, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4361 : Training loss: 0.440185546875, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4371 : Training loss: 0.518564522266, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4381 : Training loss: 0.496960997581, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4391 : Training loss: 0.495551377535, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4401 : Training loss: 0.493839472532, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4411 : Training loss: 0.483566820621, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4421 : Training loss: 0.447363436222, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4431 : Training loss: 0.487783104181, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4441 : Training loss: 0.525516867638, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 4451 : Training loss: 0.423942178488, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4461 : Training loss: 0.469454228878, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4471 : Training loss: 0.481012672186, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4481 : Training loss: 0.503388464451, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4491 : Training loss: 0.477403521538, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4501 : Training loss: 0.45612090826, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4511 : Training loss: 0.512575447559, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4521 : Training loss: 0.47960627079, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4531 : Training loss: 0.469964087009, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4541 : Training loss: 0.455540776253, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4551 : Training loss: 0.490922957659, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4561 : Training loss: 0.497162163258, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4571 : Training loss: 0.495264589787, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4581 : Training loss: 0.472310274839, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4591 : Training loss: 0.494942486286, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4601 : Training loss: 0.522546350956, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4611 : Training loss: 0.491932779551, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4621 : Training loss: 0.483473718166, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4631 : Training loss: 0.470119416714, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4641 : Training loss: 0.440945923328, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4651 : Training loss: 0.503126621246, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4661 : Training loss: 0.456955462694, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4671 : Training loss: 0.58563876152, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4681 : Training loss: 0.486101537943, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4691 : Training loss: 0.414012044668, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4701 : Training loss: 0.450306326151, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4711 : Training loss: 0.539581239223, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4721 : Training loss: 0.442197144032, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4731 : Training loss: 0.491660028696, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4741 : Training loss: 0.49211165309, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4751 : Training loss: 0.448205262423, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4761 : Training loss: 0.476861447096, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4771 : Training loss: 0.478534013033, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4781 : Training loss: 0.475015610456, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4791 : Training loss: 0.462872117758, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4801 : Training loss: 0.400776803493, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4811 : Training loss: 0.528398275375, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4821 : Training loss: 0.444914579391, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4831 : Training loss: 0.490899801254, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4841 : Training loss: 0.480942964554, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4851 : Training loss: 0.460425108671, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4861 : Training loss: 0.465108335018, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4871 : Training loss: 0.483148366213, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4881 : Training loss: 0.550799012184, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4891 : Training loss: 0.575493037701, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4901 : Training loss: 0.471054375172, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4911 : Training loss: 0.489831000566, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4921 : Training loss: 0.511512875557, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4931 : Training loss: 0.519863724709, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4941 : Training loss: 0.491863518953, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4951 : Training loss: 0.493174165487, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4961 : Training loss: 0.470610529184, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4971 : Training loss: 0.412418752909, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4981 : Training loss: 0.489101797342, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 4991 : Training loss: 0.522384047508, \n",
      " test accuracy : 0.785714268684\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e19054090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "mlp.train(X=X_train_scaled, y=y_train, X_test=X_test_scaled, y_test=y_test, num_epochs=5000, lr=0.0001, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation to choose parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from 'model.pyc'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helper\n",
    "import model\n",
    "reload(helper)\n",
    "reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter set 1:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.728571428571\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.735714285714\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.742857142857\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.778571428571\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.739130434783\n",
      "\n",
      "Parameter set 2:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.714285714286\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.721428571429\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.75\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.785714285714\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.688405797101\n",
      "\n",
      "Parameter set 3:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.7\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.7\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.742857142857\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.785714285714\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.724637681159\n",
      "\n",
      "Parameter set 4:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.735714285714\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.728571428571\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.75\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.821428571429\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.746376811594\n",
      "\n",
      "Parameter set 5:\n",
      "Cross validation set: 1\n",
      "Accuracy: 0.742857142857\n",
      "Cross validation set: 2\n",
      "Accuracy: 0.75\n",
      "Cross validation set: 3\n",
      "Accuracy: 0.742857142857\n",
      "Cross validation set: 4\n",
      "Accuracy: 0.785714285714\n",
      "Cross validation set: 5\n",
      "Accuracy: 0.782608695652\n",
      "\n",
      "The best cross validation accuracy is: 0.760807453416\n",
      "The parameters which achieve this accuracy are: \n",
      "0: {'activations': ['elu', 'elu'], 'dropout': [0.7, 0.7], 'neurons': [30, 20], 'learning_rate': 0.0003}\n"
     ]
    }
   ],
   "source": [
    "neurons=[[30,20,10],\n",
    "         [50,40,30,20,10],\n",
    "         [30,25,20,15],\n",
    "        [50,30,40],\n",
    "        [30,20],\n",
    "        [40,35,30,25,20,15,10],\n",
    "        [20,10,5,3]]\n",
    "\n",
    "activations=[['relu']*3,\n",
    "             ['relu6']*5,\n",
    "             ['relu6']*4,\n",
    "             ['sigmoid']*3,\n",
    "             ['elu']*2,\n",
    "             ['relu6']*7,\n",
    "             ['tanh']*4\n",
    "            ]\n",
    "\n",
    "dropout = [[0.8]*3,\n",
    "           [0.8]*5,\n",
    "           [0.8]*4,\n",
    "           [0.5]*3,\n",
    "           [0.7]*2,\n",
    "           [0.6]*7,\n",
    "           [0.9]*4\n",
    "          ]\n",
    "\n",
    "lr = [0.0001, 0.001, 0.0005, 0.0001, 0.0003]\n",
    "\n",
    "helper.CV_pipeline(X_train_scaled, y_train, neurons, activations, dropout,\n",
    "                   lr, inputDim=X_train_scaled.shape[1], outputDim=2, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Full Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training loss: 0.696821451187, \n",
      " test accuracy : 0.62857145071\n",
      "\n",
      "Epoch 11 : Training loss: 0.637074351311, \n",
      " test accuracy : 0.742857158184\n",
      "\n",
      "Epoch 21 : Training loss: 0.588024377823, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 31 : Training loss: 0.530449688435, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 41 : Training loss: 0.538815677166, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 51 : Training loss: 0.48825237155, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 61 : Training loss: 0.538116514683, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 71 : Training loss: 0.493172138929, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 81 : Training loss: 0.499595880508, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 91 : Training loss: 0.456884384155, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 101 : Training loss: 0.450441122055, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 111 : Training loss: 0.500129282475, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 121 : Training loss: 0.466135680676, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 131 : Training loss: 0.445730924606, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 141 : Training loss: 0.478201419115, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 151 : Training loss: 0.457289397717, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 161 : Training loss: 0.473058670759, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 171 : Training loss: 0.450153380632, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 181 : Training loss: 0.50995182991, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 191 : Training loss: 0.488753378391, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 201 : Training loss: 0.513147592545, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 211 : Training loss: 0.485948979855, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 221 : Training loss: 0.488918602467, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 231 : Training loss: 0.479189246893, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 241 : Training loss: 0.451191395521, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 251 : Training loss: 0.45743611455, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 261 : Training loss: 0.454828381538, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 271 : Training loss: 0.478547483683, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 281 : Training loss: 0.498116880655, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 291 : Training loss: 0.481000602245, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 301 : Training loss: 0.476238906384, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 311 : Training loss: 0.462356358767, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 321 : Training loss: 0.473681420088, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 331 : Training loss: 0.437957376242, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 341 : Training loss: 0.465714395046, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 351 : Training loss: 0.487808853388, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 361 : Training loss: 0.48449999094, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 371 : Training loss: 0.470763117075, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 381 : Training loss: 0.484702736139, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 391 : Training loss: 0.445079594851, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 401 : Training loss: 0.504905879498, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 411 : Training loss: 0.479339927435, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 421 : Training loss: 0.487192153931, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 431 : Training loss: 0.442768126726, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 441 : Training loss: 0.48497325182, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 451 : Training loss: 0.429284691811, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 461 : Training loss: 0.476933479309, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 471 : Training loss: 0.43985965848, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 481 : Training loss: 0.466750025749, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 491 : Training loss: 0.451611340046, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 501 : Training loss: 0.442715227604, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 511 : Training loss: 0.464106142521, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 521 : Training loss: 0.45326936245, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 531 : Training loss: 0.479580491781, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 541 : Training loss: 0.463056743145, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 551 : Training loss: 0.453068315983, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 561 : Training loss: 0.446765631437, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 571 : Training loss: 0.473994135857, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 581 : Training loss: 0.469231784344, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 591 : Training loss: 0.466201901436, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 601 : Training loss: 0.454182505608, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 611 : Training loss: 0.475468218327, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 621 : Training loss: 0.453317910433, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 631 : Training loss: 0.44964030385, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 641 : Training loss: 0.435093969107, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 651 : Training loss: 0.465939939022, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 661 : Training loss: 0.428304225206, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 671 : Training loss: 0.432330697775, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 681 : Training loss: 0.433078974485, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 691 : Training loss: 0.476129412651, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 701 : Training loss: 0.452740371227, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 711 : Training loss: 0.455189973116, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 721 : Training loss: 0.447731792927, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 731 : Training loss: 0.44054672122, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 741 : Training loss: 0.476758211851, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 751 : Training loss: 0.459183990955, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 761 : Training loss: 0.44755667448, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 771 : Training loss: 0.43515175581, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 781 : Training loss: 0.45083540678, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 791 : Training loss: 0.45151335001, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 801 : Training loss: 0.462240219116, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 811 : Training loss: 0.440142035484, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 821 : Training loss: 0.463015317917, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 831 : Training loss: 0.482673734426, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 841 : Training loss: 0.474362164736, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 851 : Training loss: 0.464287191629, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 861 : Training loss: 0.463023811579, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 871 : Training loss: 0.407533019781, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 881 : Training loss: 0.459303766489, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 891 : Training loss: 0.434647470713, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 901 : Training loss: 0.437724977732, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 911 : Training loss: 0.44074934721, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 921 : Training loss: 0.432918310165, \n",
      " test accuracy : 0.771428585052\n",
      "\n",
      "Epoch 931 : Training loss: 0.476148992777, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 941 : Training loss: 0.457872331142, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 951 : Training loss: 0.468669325113, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 961 : Training loss: 0.424838334322, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 971 : Training loss: 0.47502887249, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 981 : Training loss: 0.437945604324, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 991 : Training loss: 0.465554744005, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1001 : Training loss: 0.459072500467, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1011 : Training loss: 0.453291475773, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1021 : Training loss: 0.448000520468, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1031 : Training loss: 0.445497900248, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1041 : Training loss: 0.455367207527, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1051 : Training loss: 0.429645985365, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1061 : Training loss: 0.475969642401, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1071 : Training loss: 0.480163156986, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1081 : Training loss: 0.454800873995, \n",
      " test accuracy : 0.785714268684\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1091 : Training loss: 0.458276242018, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1101 : Training loss: 0.450974851847, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1111 : Training loss: 0.444356054068, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1121 : Training loss: 0.4177493155, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1131 : Training loss: 0.482192724943, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1141 : Training loss: 0.408929228783, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1151 : Training loss: 0.431483656168, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1161 : Training loss: 0.417974084616, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1171 : Training loss: 0.436464548111, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1181 : Training loss: 0.424119055271, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1191 : Training loss: 0.427218496799, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1201 : Training loss: 0.425834566355, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1211 : Training loss: 0.435747116804, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1221 : Training loss: 0.452294766903, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1231 : Training loss: 0.429644405842, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1241 : Training loss: 0.422400712967, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1251 : Training loss: 0.412518471479, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1261 : Training loss: 0.432027280331, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1271 : Training loss: 0.440404713154, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1281 : Training loss: 0.422511458397, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1291 : Training loss: 0.448532015085, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1301 : Training loss: 0.454653441906, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1311 : Training loss: 0.417137086391, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1321 : Training loss: 0.484897106886, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1331 : Training loss: 0.438443094492, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1341 : Training loss: 0.442170202732, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1351 : Training loss: 0.426127612591, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1361 : Training loss: 0.426193594933, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1371 : Training loss: 0.456521332264, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1381 : Training loss: 0.456485688686, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1391 : Training loss: 0.420853406191, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1401 : Training loss: 0.431571573019, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1411 : Training loss: 0.468992888927, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1421 : Training loss: 0.472984492779, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1431 : Training loss: 0.47217425704, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1441 : Training loss: 0.452965527773, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1451 : Training loss: 0.428524702787, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1461 : Training loss: 0.398146420717, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1471 : Training loss: 0.431816518307, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1481 : Training loss: 0.421446442604, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1491 : Training loss: 0.429311752319, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1501 : Training loss: 0.44508677721, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1511 : Training loss: 0.421936929226, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1521 : Training loss: 0.418758779764, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1531 : Training loss: 0.453244715929, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1541 : Training loss: 0.45247977972, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1551 : Training loss: 0.433383733034, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1561 : Training loss: 0.447700113058, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1571 : Training loss: 0.414487421513, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1581 : Training loss: 0.453835755587, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1591 : Training loss: 0.426473617554, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1601 : Training loss: 0.402783095837, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1611 : Training loss: 0.444065660238, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1621 : Training loss: 0.405337095261, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1631 : Training loss: 0.461446166039, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1641 : Training loss: 0.430663406849, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1651 : Training loss: 0.424913525581, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1661 : Training loss: 0.419425576925, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1671 : Training loss: 0.463468760252, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1681 : Training loss: 0.426552414894, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1691 : Training loss: 0.443964749575, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1701 : Training loss: 0.425209224224, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1711 : Training loss: 0.422935217619, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1721 : Training loss: 0.424546539783, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1731 : Training loss: 0.450081795454, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1741 : Training loss: 0.472944378853, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1751 : Training loss: 0.394807934761, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1761 : Training loss: 0.453764170408, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1771 : Training loss: 0.4408159554, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1781 : Training loss: 0.414003908634, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1791 : Training loss: 0.40232282877, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1801 : Training loss: 0.429725289345, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1811 : Training loss: 0.429708808661, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1821 : Training loss: 0.41900202632, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1831 : Training loss: 0.46281182766, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1841 : Training loss: 0.414355158806, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1851 : Training loss: 0.402977079153, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1861 : Training loss: 0.416402429342, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1871 : Training loss: 0.379109948874, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1881 : Training loss: 0.392066687346, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1891 : Training loss: 0.421938866377, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1901 : Training loss: 0.407324969769, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1911 : Training loss: 0.41272482276, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1921 : Training loss: 0.444205164909, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1931 : Training loss: 0.405199974775, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1941 : Training loss: 0.395432680845, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1951 : Training loss: 0.451272577047, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1961 : Training loss: 0.413663983345, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1971 : Training loss: 0.417409062386, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 1981 : Training loss: 0.393001943827, \n",
      " test accuracy : 0.785714268684\n",
      "\n",
      "Epoch 1991 : Training loss: 0.435415565968, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2001 : Training loss: 0.435281008482, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2011 : Training loss: 0.40233951807, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2021 : Training loss: 0.393945783377, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2031 : Training loss: 0.417153477669, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2041 : Training loss: 0.417107224464, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2051 : Training loss: 0.468874245882, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2061 : Training loss: 0.436869949102, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2071 : Training loss: 0.434120446444, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2081 : Training loss: 0.404300004244, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2091 : Training loss: 0.423717290163, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2101 : Training loss: 0.39925840497, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2111 : Training loss: 0.427188634872, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2121 : Training loss: 0.424006074667, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2131 : Training loss: 0.421499609947, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2141 : Training loss: 0.446797609329, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2151 : Training loss: 0.420113384724, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2161 : Training loss: 0.405894011259, \n",
      " test accuracy : 0.800000011921\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2171 : Training loss: 0.405516415834, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2181 : Training loss: 0.439760833979, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2191 : Training loss: 0.438668310642, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2201 : Training loss: 0.44298183918, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2211 : Training loss: 0.424534857273, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2221 : Training loss: 0.426195770502, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2231 : Training loss: 0.425915628672, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2241 : Training loss: 0.430623650551, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2251 : Training loss: 0.402912855148, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2261 : Training loss: 0.43694537878, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2271 : Training loss: 0.421227753162, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2281 : Training loss: 0.439291298389, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2291 : Training loss: 0.449060112238, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2301 : Training loss: 0.436524987221, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2311 : Training loss: 0.419637680054, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2321 : Training loss: 0.453295767307, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2331 : Training loss: 0.408461838961, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2341 : Training loss: 0.388879835606, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2351 : Training loss: 0.445475697517, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2361 : Training loss: 0.472761869431, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2371 : Training loss: 0.393197506666, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2381 : Training loss: 0.446696519852, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2391 : Training loss: 0.400048702955, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2401 : Training loss: 0.4180970788, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2411 : Training loss: 0.407461076975, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 2421 : Training loss: 0.391295850277, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2431 : Training loss: 0.44118437171, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2441 : Training loss: 0.426881670952, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2451 : Training loss: 0.428929537535, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2461 : Training loss: 0.398516386747, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2471 : Training loss: 0.407409787178, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2481 : Training loss: 0.438001990318, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2491 : Training loss: 0.435250371695, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2501 : Training loss: 0.404882371426, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2511 : Training loss: 0.428997725248, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2521 : Training loss: 0.384130507708, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2531 : Training loss: 0.421638131142, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2541 : Training loss: 0.391504317522, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2551 : Training loss: 0.417240083218, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2561 : Training loss: 0.403384506702, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2571 : Training loss: 0.419967889786, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2581 : Training loss: 0.418508887291, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2591 : Training loss: 0.443025350571, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2601 : Training loss: 0.421995609999, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2611 : Training loss: 0.40746307373, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2621 : Training loss: 0.42242115736, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2631 : Training loss: 0.435176432133, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2641 : Training loss: 0.408913165331, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2651 : Training loss: 0.455796599388, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2661 : Training loss: 0.41665789485, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2671 : Training loss: 0.428541839123, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2681 : Training loss: 0.439043551683, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2691 : Training loss: 0.418253600597, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2701 : Training loss: 0.425773590803, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2711 : Training loss: 0.405890703201, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2721 : Training loss: 0.416691809893, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2731 : Training loss: 0.420091122389, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2741 : Training loss: 0.418164134026, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2751 : Training loss: 0.391226053238, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2761 : Training loss: 0.407025963068, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2771 : Training loss: 0.37333136797, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2781 : Training loss: 0.416764616966, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2791 : Training loss: 0.405799359083, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2801 : Training loss: 0.38070550561, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2811 : Training loss: 0.423959136009, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2821 : Training loss: 0.433355122805, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2831 : Training loss: 0.437946766615, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2841 : Training loss: 0.424687802792, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2851 : Training loss: 0.37345957756, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2861 : Training loss: 0.422914803028, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2871 : Training loss: 0.415859103203, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2881 : Training loss: 0.391925632954, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2891 : Training loss: 0.412108510733, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2901 : Training loss: 0.420829862356, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2911 : Training loss: 0.438072770834, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2921 : Training loss: 0.424552172422, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2931 : Training loss: 0.399960309267, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2941 : Training loss: 0.38726708293, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2951 : Training loss: 0.388839483261, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2961 : Training loss: 0.405290216208, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2971 : Training loss: 0.429297327995, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 2981 : Training loss: 0.392624020576, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 2991 : Training loss: 0.429642111063, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3001 : Training loss: 0.41872176528, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3011 : Training loss: 0.416026502848, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3021 : Training loss: 0.387980639935, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3031 : Training loss: 0.401095151901, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3041 : Training loss: 0.393188089132, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3051 : Training loss: 0.428955048323, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3061 : Training loss: 0.37335973978, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3071 : Training loss: 0.397923439741, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3081 : Training loss: 0.399017095566, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3091 : Training loss: 0.402065783739, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3101 : Training loss: 0.408812224865, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3111 : Training loss: 0.389398515224, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3121 : Training loss: 0.413035303354, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3131 : Training loss: 0.391204148531, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3141 : Training loss: 0.35660341382, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3151 : Training loss: 0.382882624865, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3161 : Training loss: 0.417304754257, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3171 : Training loss: 0.38839456439, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3181 : Training loss: 0.434689193964, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3191 : Training loss: 0.455023884773, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3201 : Training loss: 0.423233538866, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3211 : Training loss: 0.427054852247, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3221 : Training loss: 0.411592155695, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3231 : Training loss: 0.41709241271, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3241 : Training loss: 0.432072937489, \n",
      " test accuracy : 0.828571438789\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3251 : Training loss: 0.374803483486, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3261 : Training loss: 0.348469346762, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3271 : Training loss: 0.385466992855, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3281 : Training loss: 0.378443747759, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3291 : Training loss: 0.431752175093, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3301 : Training loss: 0.392485052347, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3311 : Training loss: 0.376771092415, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3321 : Training loss: 0.412630289793, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3331 : Training loss: 0.395850092173, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3341 : Training loss: 0.40857937932, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 3351 : Training loss: 0.449925541878, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3361 : Training loss: 0.383903294802, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3371 : Training loss: 0.428477942944, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3381 : Training loss: 0.415426671505, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3391 : Training loss: 0.438613027334, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3401 : Training loss: 0.447496414185, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3411 : Training loss: 0.405815958977, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3421 : Training loss: 0.408800899982, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3431 : Training loss: 0.370902776718, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3441 : Training loss: 0.38766387105, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3451 : Training loss: 0.400105446577, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3461 : Training loss: 0.395961821079, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3471 : Training loss: 0.416400313377, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3481 : Training loss: 0.377394109964, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3491 : Training loss: 0.395108491182, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3501 : Training loss: 0.409730195999, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3511 : Training loss: 0.394759178162, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3521 : Training loss: 0.38995462656, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3531 : Training loss: 0.383799850941, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3541 : Training loss: 0.391639828682, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3551 : Training loss: 0.351526170969, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3561 : Training loss: 0.394852608442, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3571 : Training loss: 0.394007980824, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3581 : Training loss: 0.380813360214, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3591 : Training loss: 0.420210450888, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3601 : Training loss: 0.446771889925, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3611 : Training loss: 0.367783784866, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3621 : Training loss: 0.415852457285, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3631 : Training loss: 0.392240434885, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3641 : Training loss: 0.389699369669, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3651 : Training loss: 0.375589936972, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3661 : Training loss: 0.371154278517, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3671 : Training loss: 0.437910377979, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3681 : Training loss: 0.350261598825, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3691 : Training loss: 0.394966155291, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3701 : Training loss: 0.372938424349, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3711 : Training loss: 0.404634922743, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3721 : Training loss: 0.409043103456, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3731 : Training loss: 0.399274259806, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3741 : Training loss: 0.387281447649, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3751 : Training loss: 0.420644074678, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3761 : Training loss: 0.4126521945, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3771 : Training loss: 0.379976898432, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3781 : Training loss: 0.376316040754, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3791 : Training loss: 0.404095441103, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3801 : Training loss: 0.409960478544, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3811 : Training loss: 0.380086541176, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3821 : Training loss: 0.390455335379, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3831 : Training loss: 0.439678281546, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3841 : Training loss: 0.351628482342, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3851 : Training loss: 0.465353131294, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3861 : Training loss: 0.411017417908, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3871 : Training loss: 0.367962002754, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3881 : Training loss: 0.392448842525, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3891 : Training loss: 0.391307085752, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3901 : Training loss: 0.354053705931, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3911 : Training loss: 0.385845512152, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3921 : Training loss: 0.388937354088, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3931 : Training loss: 0.387161076069, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3941 : Training loss: 0.398902624846, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3951 : Training loss: 0.434261292219, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3961 : Training loss: 0.370606541634, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3971 : Training loss: 0.361751884222, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3981 : Training loss: 0.394167453051, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 3991 : Training loss: 0.386909782887, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4001 : Training loss: 0.395490974188, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4011 : Training loss: 0.375537216663, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4021 : Training loss: 0.358216255903, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4031 : Training loss: 0.40079754591, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4041 : Training loss: 0.371366202831, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4051 : Training loss: 0.382337838411, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4061 : Training loss: 0.357355058193, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4071 : Training loss: 0.407812446356, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4081 : Training loss: 0.394447267056, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4091 : Training loss: 0.375554263592, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4101 : Training loss: 0.374359667301, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4111 : Training loss: 0.332641035318, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4121 : Training loss: 0.424695253372, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4131 : Training loss: 0.35326397419, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4141 : Training loss: 0.395620852709, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4151 : Training loss: 0.407750070095, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4161 : Training loss: 0.420045346022, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4171 : Training loss: 0.380906134844, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4181 : Training loss: 0.397626042366, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4191 : Training loss: 0.38469183445, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4201 : Training loss: 0.350555598736, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4211 : Training loss: 0.402345895767, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4221 : Training loss: 0.376821219921, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4231 : Training loss: 0.384878367186, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4241 : Training loss: 0.380921781063, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4251 : Training loss: 0.431710928679, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4261 : Training loss: 0.369173616171, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4271 : Training loss: 0.354809194803, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4281 : Training loss: 0.374081492424, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4291 : Training loss: 0.373095333576, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4301 : Training loss: 0.34135016799, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4311 : Training loss: 0.390442728996, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4321 : Training loss: 0.374361246824, \n",
      " test accuracy : 0.814285695553\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4331 : Training loss: 0.429102122784, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4341 : Training loss: 0.38690033555, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4351 : Training loss: 0.381700634956, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4361 : Training loss: 0.354809761047, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4371 : Training loss: 0.40609267354, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4381 : Training loss: 0.368986904621, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4391 : Training loss: 0.397753089666, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4401 : Training loss: 0.384487509727, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4411 : Training loss: 0.39341044426, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4421 : Training loss: 0.367445886135, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4431 : Training loss: 0.352535456419, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4441 : Training loss: 0.373326808214, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4451 : Training loss: 0.38854894042, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4461 : Training loss: 0.371134102345, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4471 : Training loss: 0.360309541225, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4481 : Training loss: 0.36255928874, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4491 : Training loss: 0.344102859497, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4501 : Training loss: 0.351830154657, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4511 : Training loss: 0.387202143669, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4521 : Training loss: 0.388218611479, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4531 : Training loss: 0.39330008626, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4541 : Training loss: 0.37523329258, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4551 : Training loss: 0.37868231535, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4561 : Training loss: 0.390562832355, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 4571 : Training loss: 0.376411408186, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4581 : Training loss: 0.413777798414, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 4591 : Training loss: 0.396466642618, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 4601 : Training loss: 0.333687156439, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 4611 : Training loss: 0.32751095295, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4621 : Training loss: 0.346187204123, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 4631 : Training loss: 0.401743203402, \n",
      " test accuracy : 0.828571438789\n",
      "\n",
      "Epoch 4641 : Training loss: 0.365763247013, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4651 : Training loss: 0.354238122702, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4661 : Training loss: 0.337654650211, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4671 : Training loss: 0.432869940996, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4681 : Training loss: 0.405030250549, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4691 : Training loss: 0.332719624043, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4701 : Training loss: 0.361351400614, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4711 : Training loss: 0.389492630959, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4721 : Training loss: 0.369824022055, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4731 : Training loss: 0.377344578505, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4741 : Training loss: 0.363624125719, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4751 : Training loss: 0.310371607542, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4761 : Training loss: 0.366385281086, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4771 : Training loss: 0.363844156265, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4781 : Training loss: 0.391519010067, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4791 : Training loss: 0.356746077538, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4801 : Training loss: 0.40917557478, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4811 : Training loss: 0.369130820036, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4821 : Training loss: 0.392352372408, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4831 : Training loss: 0.399154543877, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4841 : Training loss: 0.407888323069, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4851 : Training loss: 0.383895367384, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4861 : Training loss: 0.364974349737, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4871 : Training loss: 0.430069863796, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4881 : Training loss: 0.355868428946, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4891 : Training loss: 0.376601964235, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4901 : Training loss: 0.40103250742, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4911 : Training loss: 0.398523628712, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4921 : Training loss: 0.397079855204, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4931 : Training loss: 0.376434862614, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4941 : Training loss: 0.348750650883, \n",
      " test accuracy : 0.800000011921\n",
      "\n",
      "Epoch 4951 : Training loss: 0.418896317482, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4961 : Training loss: 0.366935998201, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4971 : Training loss: 0.34320768714, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4981 : Training loss: 0.376935452223, \n",
      " test accuracy : 0.814285695553\n",
      "\n",
      "Epoch 4991 : Training loss: 0.347961425781, \n",
      " test accuracy : 0.814285695553\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3298b9b950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputDim = X_train_scaled.shape[1]\n",
    "outputDim = y_train.shape[1]\n",
    "mlp = MLP()\n",
    "mlp.buildModel(neurons=[30,20], activations=['elu', 'elu'], \n",
    "               dropout=[0.7,0.7], inputDim=inputDim, outputDim=outputDim)\n",
    "\n",
    "%matplotlib inline\n",
    "results = mlp.train(X=X_train_scaled, y=y_train, X_test=X_test_scaled, y_test=y_test, num_epochs=5000, lr=0.0003, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on each test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_scaled_list = data.returnScaledTestList(scaler=scale_X, test_list=test_split_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on each scaled test set and obtain average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy on imputed test sets: 0.814285714286\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = list()\n",
    "\n",
    "for ts in test_split_scaled_list:\n",
    "    res = mlp.predict(X=ts[0])\n",
    "    accuracy = (res[\"y_pred_cls\"][:,0] == ts[1][:,0]).sum() / float(len(res[\"y_pred_cls\"]))\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "print(\"The average accuracy on imputed test sets: {}\".format(np.mean(accuracy_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
